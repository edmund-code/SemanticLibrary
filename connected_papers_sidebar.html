<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 100vh;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#4d8d8c", "custom_abstract": "Large-scale artificial intelligence foundation models have emerged as promising tools for addressing healthcare challenges, including digital pathology. While many have been developed for complex tasks such as disease diagnosis and tissue quantification using extensive and diverse datasets, their readiness for seemingly simpler tasks, such as nuclei segmentation within a single organ (for example, the kidney), remains unclear. This study answers two questions: How good are current cell foundation models? and How can we improve them? We curated a multi-center, multi-disease, and multi-species dataset sampled from 2542 kidney whole slide images. Three state-of-the-art cell foundation models\u2014Cellpose, StarDist, and CellViT\u2014were evaluated. To enhance performance, we developed a human-in-the-loop strategy that distilled multi-model predictions, improving data quality while reducing reliance on pixel-level annotation. Fine-tuning was performed using the enriched datasets, and segmentation performance was quantitatively assessed. Here we show that cell nuclei segmentation in kidney pathology still requires improvement with more organ-targeted foundation models. Among the evaluated models, CellViT achieves the highest baseline performance, with an F1 score of 0.78. Fine-tuning with enriched data improves all three models, with StarDist achieving the highest F1 score of 0.82. The combination of the foundation model-generated pseudo-labels and a subset of pathologist-corrected \u201chard\u201d patches yields consistent performance gains across all models. This study establishes a benchmark for the development and deployment of cell AI foundation models tailored to real-world data. The proposed framework, which leverages foundation models with reduced expert annotation, supports more efficient workflows in clinical pathology.", "custom_author": "Junlin Guo, Siqi Lu, Can Cui, Ruining Deng, Tianyuan Yao, Zhewen Tao, Yizhe Lin, Marilyn Lionts, Quan Liu, Juming Xiong, Yu Wang, Shilin Zhao, Catie Chang, Mitchell Wilkes, Agnes Fogo, Mengmeng Yin, Haichun Yang, Yuankai Huo", "custom_title": "Evaluating cell AI foundation models in kidney pathology with human-in-the-loop enrichment", "custom_year": "2025", "font": {"color": "#343434"}, "id": 0, "label": "Evaluating cell AI foundation", "shape": "dot", "size": 87}, {"color": "#4d8d8c", "custom_abstract": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.", "custom_author": "A Preprint, Lin Bai, Xiaoyang Li, Liqiang Huang, Quynh Nguyen, Hien Van Nguyen, Saurabh Prasad, Dragan Maric, John Redell, Pramod Dash, Badrinath Roysam", "custom_title": "WEAK-TO-STRONG GENERALIZATION ENABLES FULLY AUTOMATED DE NOVO TRAINING OF MULTI-HEAD MASK-RCNN MODEL FOR SEGMENTING DENSELY OVERLAPPING CELL NUCLEI IN MULTIPLEX WHOLE-SLICE BRAIN IMAGES", "custom_year": "2025", "font": {"color": "#343434"}, "id": 1, "label": "WEAK-TO-STRONG GENERALIZATION ENABLES FULLY", "shape": "dot", "size": 93}, {"color": "#4d8d8c", "custom_abstract": "The development of spatial transcriptomics (ST) technologies has transformed genetic research from a single-cell data level to a two-dimensional spatial coordinate system and facilitated the study of the composition and function of various cell subsets in different environments and organs. The large-scale data generated by these ST technologies, which contain spatial gene expression information, have elicited the need for spatially resolved approaches to meet the requirements of computational and biological data interpretation. These requirements include dealing with the explosive growth of data to determine the cell-level and gene-level expression, correcting the inner batch effect and loss of expression to improve the data quality, conducting efficient interpretation and in-depth knowledge mining both at the single-cell and tissue-wide levels, and conducting multi-omics integration analysis to provide an extensible framework toward the in-depth understanding of biological processes. However, algorithms designed specifically for ST technologies to meet these requirements are still in their infancy. Here, we review computational approaches to these problems in light of corresponding issues and challenges, and present forward-looking insights into algorithm development.", "custom_author": "Shuangsang Fang, Bichao Chen, Yong Zhang, Haixi Sun, Longqi Liu, Shiping Liu, Yuxiang Li, Xun Xu", "custom_title": "Computational Approaches and Challenges in Spatial Transcriptomics", "custom_year": "2022", "font": {"color": "#343434"}, "id": 2, "label": "Computational Approaches and Challenges", "shape": "dot", "size": 75}, {"color": "#4d8d8c", "custom_abstract": "Computing object skeletons in natural images is challenging, owing to large variations in object appearance and scale, and the complexity of handling background cluter. Many recent methods frame object skeleton detection as a binary pixel classification problem, which is similar in spirit to learning-based edge detection, as well as to semantic segmentation methods. In the present article, we depart from this strategy by training a CNN to predict a two-dimensional vector field, which maps each scene point to a candidate skeleton pixel, in the spirit of flux-based skeletonization algorithms. This \u201cimage context flux\u201d representation has two major advantages over previous approaches. First, it explicitly encodes the relative position of skeletal pixels to semantically meaningful entities, such as the image points in their spatial context, and hence also the implied object boundaries. Second, since the skeleton detection context is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method on three benchmark datasets for skeleton detection and two for symmetry detection, achieving consistently superior performance over state-of-the-art methods. The code is available at https://github.com/YukangWang/DeepFlux.", "custom_author": "Yukang Wang, Yongchao Xu, Stavros Tsogkas, Xiang Bai, Sven Dickinson, Kaleem Siddiqi", "custom_title": "DeepFlux for Skeletons in the Wild", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 3, "label": "DeepFlux for Skeletons in", "shape": "dot", "size": 63}, {"color": "#4d8d8c", "custom_abstract": "A principal challenge in the analysis of tissue imaging data is cell segmentation-the task of identifying the precise boundary of every cell in an image. To address this problem we constructed TissueNet, a dataset for training segmentation models that contains more than 1 million manually labeled cells, an order of magnitude more than all previously published segmentation training datasets. We used TissueNet to train Mesmer, a deep-learning-enabled segmentation algorithm. We demonstrated that Mesmer is more accurate than previous methods, generalizes to the full diversity of tissue types and imaging platforms in TissueNet, and achieves human-level performance. Mesmer enabled the automated extraction of key cellular features, such as subcellular localization of protein signal, which was challenging with previous approaches. We then adapted Mesmer to harness cell lineage information in highly multiplexed datasets and used this enhanced version to quantify cell morphology changes during human gestation. All code, data and models are released as a community resource.", "custom_author": "Noah Greenwald, Geneva Miller, Erick Moen, Alex Kong, Adam Kagel, Thomas Dougherty, Christine Fullaway, Brianna Mcintosh, Ke Leow, Morgan Schwartz, Cole Pavelchek, Sunny Cui, Isabella Camplisson, Omer Bar-Tal, Jaiveer Singh, Mara Fong, Gautam Chaudhry, Zion Abraham, Jackson Moseley, Shiri Warshawsky, Erin Soon, Shirley Greenbaum, Tyler Risom, Travis Hollmann, Sean Bendall, Leeat Keren, William Graf, Michael Angelo, David Van Valen", "custom_title": "Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning", "custom_year": "2021", "font": {"color": "#343434"}, "id": 4, "label": "Whole-cell segmentation of tissue", "shape": "dot", "size": 72}, {"color": "#4d8d8c", "custom_abstract": "Spatial transcriptomics (ST) faces persistent challenges in cell segmentation accuracy, which can bias biological interpretations in a spatial-dependent way. FastReseg introduces a novel algorithm that refines inaccuracies in existing image-based segmentations using transcriptomic data, without radically redefining cell boundaries. By combining image-based information with 3D transcriptomic precision, FastReseg enhances segmentation accuracy. Its key innovation, a transcript scoring system based on log-likelihood ratios, facilitates the quick identification and correction of spatial doublets caused by cell proximity or overlap in 2D. FastReseg reduces circularity in boundary derivation, and addresses computational challenges with a modular workflow designed for large datasets. The algorithm\u0027s modularity allows for seamless optimization and integration of advancements in segmentation technology. FastReseg provides a scalable, efficient solution to improve the quality and interpretability of ST data, ensuring compatibility with evolving segmentation methods and enabling more accurate biological insights. Accurate cell segmentation is foundational in spatial transcriptomics, as it delineates individual cells and assigns transcript molecules to their cellular origins, influencing all subsequent analyses. Segmentation errors can contaminate single-cell expression profiles with neighboring cell transcripts, introducing spatially dependent bias that confounds analyses. For instance, differential expression analyses of a given cell type across spatial domains may be skewed by errors that arise from poorly segmented cells, overshadowing genuine biological signals 1 . Most spatial transcriptomics studies (Fig. 1A) employ image-based cell segmentation, which involves immunofluorescence (IF) staining of cellular morphological markers like the nucleus and membrane to identify cell boundaries. Classical methods like the watershed algorithm 2 and nucleus expansion have been largely superseded by more advanced deep-learning techniques such as Cellpose 3 and Mesmer 4 . However, limitations in image quality of real-world tissue samples (Fig. 1B) -such as weak staining, ambiguous boundaries, or poor 3D resolution-can lead to segmentation inaccuracies that propagate downstream. Transcript-based methods offer an alternative by segmenting regions of distinct local expression profiles 5,6 . But they can face difficulties at the borders of closely related cell types or in low-transcript-density regions, limiting their usage in many cell-driven biological studies. Baysor 7 , JSTA 8 and Proseg 9 represent a new class of hybrid approach which combine transcript data with image priors. However, in practice the image priors are usually limited to the nuclear segmentation results, and the transcriptomic data frequently dominates the final segmentation outcome. This strong dependence on transcript data risks introducing circularity to analyses, with the gene expression patterns we wish to study influencing how the single cell expression matrix is defined in the first place. Moreover, the existing hybrid methods often require substantial computational resources 7-10 due to the volume of transcript data and sophisticated modeling involved, and thus struggle to generate satisfactory cell-level segmentation for densely packed tissue samples. To address these challenges, we introduce FastReseg, an R package designed to enhance the precision of cell segmentation by leveraging transcriptomic data to correct and refine initial image-based segmentation results. FastReseg processes spatial transcriptomic datasets, along with their initial cell assignment provided by image-based cell segmentation, through a three-tiered modular approach: it first assesses cells for potential segmentation inaccuracies, then identifies misassigned transcript groups within those likely erroneous cells, and finally reassigns these mislocated transcripts to their appropriate cellular origins. This stepwise progression allows users the flexibility to halt the process after any stage to examine intermediate results, adapting the workflow to suit specific research needs at various analytical depths. The modular structure also simplifies the parameter optimization for each module, facilitating the adaptation of this tool in diverse research contexts.", "custom_author": "Lidan Wu, Joseph Beechem, Patrick Danaher", "custom_title": "Using transcripts to refine image based cell segmentation with FastReseg", "custom_year": "2025", "font": {"color": "#343434"}, "id": 5, "label": "Using transcripts to refine", "shape": "dot", "size": 69}, {"color": "#4d8d8c", "custom_abstract": "With the development of spatial transcriptomics, now we could have a much better understanding of the spatial genomic profile of complex tissue. However, the cell segmentation remains a critical step for data analysis processing. Here we developed an adaptive SAM model for cell segmentation in the spatial transcriptomics. SAM model has been developed as a powerful basic model for segmentation. In the application of biomedical image segmentation, SAM did not perform equally well for different dimension of images. In our study, we first exhibited that SAM model segmentation is sensitive to image dimension. We provided minimal human annotation to initiate the optimization for finding the best dimension images for SAM model. We found that adaptiveSAM could perform better or equally well in the biomedical image segmentation without finetuning, compared with Faster RCNN and Mask RCNN based detection head. \u2022 Computing methodologies \u2192 Supervised learning.", "custom_author": "Yuxin Pu", "custom_title": "Adaptive Segment Anything Model for Spatial Transcriptomic Cell Segmentation", "custom_year": "2024", "font": {"color": "#343434"}, "id": 6, "label": "Adaptive Segment Anything Model", "shape": "dot", "size": 78}, {"color": "#4d8d8c", "custom_abstract": "Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans. Segmentation is a fundamental task in medical imaging analysis, which involves identifying and delineating regions of interest (ROI) in various medical images, such as organs, lesions, and tissues 1 . Accurate segmentation is essential for many clinical applications, including disease diagnosis, treatment planning, and monitoring of disease progression 2,3 . Manual segmentation has long been the gold standard for delineating anatomical structures and pathological regions, but this process is time-consuming, labor-intensive, and often requires a high degree of expertise. Semi-or fully automatic segmentation methods can significantly reduce the time and labor required, increase consistency, and enable the analysis of large-scale datasets 4 . Deep learning-based models have shown great promise in medical image segmentation due to their ability to learn intricate image features and deliver accurate segmentation results across a diverse range of tasks, from segmenting specific anatomical structures to identifying pathological regions 5 . However, a significant limitation of many current medical image segmentation models is their task-specific nature. These models are typically designed and trained for a specific segmentation task, and their performance can degrade significantly when applied to new tasks or different types of imaging data 6 . This lack of generality poses a substantial obstacle to the wider application of these models in clinical practice. In contrast, recent advances in the field of natural image segmentation have witnessed the emergence of segmentation foundation models, such as segment anything model (SAM) 7 and Segment Everything Everywhere with Multi-modal prompts all at once 8 , showcasing remarkable versatility and performance across various segmentation tasks. There is a growing demand for universal models in medical image segmentation: models that can be trained once and then applied to a wide range of segmentation tasks. Such models would not only exhibit heightened versatility in terms of model capacity but also potentially lead to more consistent results across different tasks. However, the applicability of the segmentation foundation models (e.g., SAM 7 ) to medical image segmentation remains limited due to the significant differences between natural images and medical images. Essentially, SAM is a promptable segmentation method that requires points or bounding boxes to specify the segmentation targets. This resembles conventional interactive segmentation methods 4,9-11 but SAM has better generalization ability, while existing deep learning-based interactive segmentation methods focus mainly on limited tasks and image modalities. Many studies have applied the out-of-the-box SAM models to typical medical image segmentation tasks [12][13][14][15][16][17] and other challenging scenarios [18][19][20][21] . For example, the concurrent studies 22,23 conducted a", "custom_author": "Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, Bo Wang", "custom_title": "Segment anything in medical images", "custom_year": "2024", "font": {"color": "#343434"}, "id": 7, "label": "Segment anything in medical", "shape": "dot", "size": 87}, {"color": "#4d8d8c", "custom_abstract": "The emergence of large models, also known as foundation models, has brought significant advancements to AI research. One such model is Segment Anything (SAM), which is designed for image segmentation tasks. However, as with other foundation models, our experimental findings suggest that SAM may fail or perform poorly in certain segmentation tasks, such as shadow detection and camouflaged object detection (concealed object detection). This study first paves the way for applying the large pre-trained image segmentation model SAM to these downstream tasks, even in situations where SAM performs poorly. Rather than finetuning the SAM network, we propose SAM-Adapter, which incorporates domain-specific information or visual prompts into the segmentation network by using simple yet effective adapters. By integrating task-specific knowledge with general knowledge learnt by the large model, SAM-Adapter can significantly elevate the performance of SAM in challenging tasks as shown in extensive experiments. We can even outperform task-specific network models and achieve stateof-the-art performance in the task we tested: camouflaged object detection, shadow detection. Our code of adapting SAM in downstream applications have been released publicly at https://github.com/tianrun-chen/SAM-Adapter-PyTorch/ and has benefited many researchers. We believe our work opens up opportunities for utilizing SAM in downstream tasks, with potential applications in various fields, including medical image processing, agriculture, remote sensing, and more.", "custom_author": "Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, Papa Mao", "custom_title": "SAM-Adapter: Adapting Segment Anything in Underperformed Scenes", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 8, "label": "SAM-Adapter: Adapting Segment Anything", "shape": "dot", "size": 78}, {"color": "#4d8d8c", "custom_abstract": "Tubular structure segmentation in medical images, e.g., segmenting vessels in CT scans, serves as a vital step in the use of computers to aid in screening early stages of related diseases. But automatic tubular structure segmentation in CT scans is a challenging problem, due to issues such as poor contrast, noise and complicated background. A tubular structure usually has a cylinder-like shape which can be well represented by its skeleton and cross-sectional radii (scales). Inspired by this, we propose a geometryaware tubular structure segmentation method, Deep Distance Transform (DDT), which combines intuitions from the classical distance transform for skeletonization and modern deep segmentation networks. DDT first learns a multitask network to predict a segmentation mask for a tubular structure and a distance map. Each value in the map represents the distance from each tubular structure voxel to the tubular structure surface. Then the segmentation mask is refined by leveraging the shape prior reconstructed from the distance map. We apply our DDT on six medical image datasets. Results show that (1) DDT can boost tubular structure segmentation performance significantly (e.g., over 13% DSC improvement for pancreatic duct segmentation), and (2) DDT additionally provides a geometrical measurement for a tubular structure, which is important for clinical diagnosis (e.g., the cross-sectional scale of a pancreatic duct can be an indicator for pancreatic cancer). * This work was done when Xu Wei and Jieneng Chen were at JHU. \u2020 Equal Contribution.", "custom_author": "Yan Wang, \u2020 Xu, Fengze Liu, Jieneng Chen, Yuyin Zhou, Wei Shen, Elliot Fishman, Alan Yuille", "custom_title": "Deep Distance Transform for Tubular Structure Segmentation in CT Scans", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 9, "label": "Deep Distance Transform for", "shape": "dot", "size": 75}, {"color": "#4d8d8c", "custom_abstract": "Artificial intelligence has revolutionized computational biology. Recent developments in omics technologies, including single-cell RNA sequencing and spatial transcriptomics, provide detailed genomic data alongside tissue histology. However, current computational models focus on either omics or image analysis, lacking their integration. To address this, we developed OmiCLIP, a visual-omics foundation model linking hematoxylin and eosin images and transcriptomics using tissue patches from Visium data. We transformed transcriptomic data into \u0027sentences\u0027 by concatenating top-expressed gene symbols from each patch. We curated a dataset of 2.2 million paired tissue images and transcriptomic data across 32 organs to train OmiCLIP integrating histology and transcriptomics. Building on OmiCLIP, our Loki platform offers five key functions: tissue alignment, annotation via bulk RNA sequencing or marker genes, cell-type decomposition, image-transcriptomics retrieval and spatial transcriptomics gene expression prediction from hematoxylin and eosin-stained images. Compared with 22 state-of-the-art models on 5 simulations, and 19 public and 4 in-house experimental datasets, Loki demonstrated consistent accuracy and robustness. Computational biology has advanced notably with artificial intelligence (AI) for tasks such as gene expression enhancement, single-cell perturbation prediction, tissue annotation, diagnosis, primary tumor origin predictions and image retrieval from hematoxylin and eosin (H\u0026E)-stained images [1][2][3][4][5][6][7] . Recently, foundation models like CLIP 8 , CoCa 9 and DeCLIP 10 have been adapted to the field, fine-tuned with pathology images and captions, as seen in PLIP and CONCH 11,12 . These visual-language foundation models support applications like text-to-image and image-to-text retrieval, histology image classification, captioning and diagnosis improvement. Omics data, including transcriptomics and genetics, provide crucial insights into cell types in health and disease, enhancing our understanding of cellular heterogeneity, lineage tracing and disease mechanisms [13][14][15][16][17][18][19][20][21][22] . Combining omics data with histology images offers complementary information for both research and clinical applications, and has been used for predicting cancer outcomes, prognosis and response to neoadjuvant chemotherapy 3 . However, existing methods remain task specific and lack a unified multimodal AI model to integrate histology and omics data. Additionally, challenges remain in developing infrastructure to efficiently analyze sequencing data and pathology images together. To address these gaps, we introduce omics and image pretraining, OmiCLIP, a transcriptomic-image dual-encoder foundation model and Loki platform, an infrastructure of multimodal analysis using OmiCLIP as a backbone. To train OmiCLIP, we curated the ST-bank dataset with 2.2 million tissue patches from 1,007 samples across 32 organs with paired whole-slide images (WSIs) and 10x Visium spatial transcriptomics (ST) data. Inspired by large language model-based single-cell models", "custom_author": "Weiqing Chen, Pengzhi Zhang, Tu Tran, Yiwei Xiao, Shengyu Li, Vrutant Shah, Hao Cheng, Kristopher Brannan, Keith Youker, Li Lai, Longhou Fang, Yu Yang, Nhat-Tu Le, Jun-Ichi Abe, Shu-Hsia Chen, Qin Ma, Ken Chen, Qianqian Song, John Cooke, Guangyu Wang", "custom_title": "A visual\u2013omics foundation model to bridge histopathology with spatial transcriptomics", "custom_year": "2025", "font": {"color": "#343434"}, "id": 10, "label": "A visual\u2013omics foundation model", "shape": "dot", "size": 90}, {"color": "#4d8d8c", "custom_abstract": "Accurate segmentation of tubular, network-like structures, such as vessels, neurons, or roads, is relevant to many fields of research. For such structures, the topology is their most important characteristic; particularly preserving connectedness: in the case of vascular networks, missing a connected vessel entirely alters the blood-flow dynamics. We introduce a novel similarity measure termed center-lineDice (short clDice), which is calculated on the intersection of the segmentation masks and their (morphological) skeleta. We theoretically prove that clDice guarantees topology preservation up to homotopy equivalence for binary 2D and 3D segmentation. Extending this, we propose a computationally efficient, differentiable loss function (soft-clDice) for training arbitrary neural segmentation networks. We benchmark the soft-clDice loss on five public datasets, including vessels, roads and neurons (2D  and 3D). Training on soft-clDice leads to segmentation with more accurate connectivity information, higher graph similarity, and better volumetric scores.", "custom_author": "Suprosanna Shit, Johannes Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien Pluim, Ulrich Bauer, Bjoern Menze", "custom_title": "clDice -a Novel Topology-Preserving Loss Function for Tubular Structure Segmentation", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 11, "label": "clDice -a Novel Topology-Preserving", "shape": "dot", "size": 87}, {"color": "#4d8d8c", "custom_abstract": "Curvilinear structure segmentation (CSS) is essential in various domains, including medical imaging, landscape analysis, industrial surface inspection, and plant analysis. While existing methods achieve high performance within specific domains, their generalizability is limited. On the other hand, large-scale models such as Segment Anything Model (SAM) exhibit strong generalization but are not optimized for curvilinear structures. Existing adaptations of SAM primarily focus on general object segmentation and lack specialized design for CSS tasks. To bridge this gap, we propose the Universal Curvilinear structure Segmentation (UCS) model, which adapts SAM to CSS tasks while further enhancing its cross-domain generalization. UCS features a novel encoder architecture integrating a pretrained SAM encoder with two innovations: a Sparse Adapter, strategically inserted to inherit the pre-trained SAM encoder\u0027s generalization capability while minimizing the number of fine-tuning parameters, and a Prompt Generation module, which leverages Fast Fourier Transform with a high-pass filter to generate curve-specific prompts. Furthermore, the UCS incorporates a mask decoder that eliminates reliance on manual interaction through a dual-compression module: a Hierarchical Feature Compression module, which aggregates the outputs of the sampled encoder to enhance detail preservation, and a Guidance Feature Compression module, which extracts and compresses image-driven guidance features. Evaluated on a comprehensive multi-domain dataset, including an in-house dataset covering eight natural curvilinear structures, UCS demonstrates state-ofthe-art generalization and open-set segmentation performance across medical, engineering, natural, and plant imagery, establishing a new benchmark for universal CSS. The source code is available at https://github.com/kylechuuuuu/UCS.", "custom_author": "Kai Zhu, Li Chen, Dianshuo Li, Yunxiang Cao, Jun Cheng", "custom_title": "UCS: Universal Model for Curvilinear Structure Segmentation", "custom_year": "2025", "font": {"color": "#343434"}, "id": 12, "label": "UCS: Universal Model for", "shape": "dot", "size": 75}, {"color": "#4d8d8c", "custom_abstract": "Modern algorithms for biological segmentation can match inter-human agreement in annotation quality. This however is not a performance bound: a hypothetical human-consensus segmentation could reduce error rates in half. To obtain a model that generalizes better we adapted the pretrained transformer backbone of a foundation model (SAM) to the Cellpose framework. The resulting Cellpose-SAM model substantially outperforms inter-human agreement and approaches the human-consensus bound. We increase generalization performance further by making the model robust to channel shuffling, cell size, shot noise, downsampling, isotropic and anisotropic blur. The new model can be readily adopted into the Cellpose ecosystem which includes finetuning, human-in-the-loop training, image restoration and 3D segmentation approaches. These properties establish Cellpose-SAM as a foundation model for biological segmentation.", "custom_author": "Marius Pachitariu, Michael Rariden, Carsen Stringer", "custom_title": "Cellpose-SAM: superhuman generalization for cellular segmentation", "custom_year": "2025", "font": {"color": "#343434"}, "id": 13, "label": "Cellpose-SAM: superhuman generalization for", "shape": "dot", "size": 93}, {"color": "#4d8d8c", "custom_abstract": "Spatial transcriptomics is a cutting-edge technique that combines gene expression with spatial information, allowing researchers to study molecular patterns within tissue architecture. Here, we present IAMSAM, a user-friendly web-based tool for analyzing spatial transcriptomics data focusing on morphological features. IAMSAM accurately segments tissue images using the Segment Anything Model, allowing for the semiautomatic selection of regions of interest based on morphological signatures. Furthermore, IAMSAM provides downstream analysis, such as identifying differentially expressed genes, enrichment analysis, and cell type prediction within the selected regions. With its simple interface, IAMSAM empowers researchers to explore and interpret heterogeneous tissues in a streamlined manner.", "custom_author": "Dongjoo Lee, Jeongbin Park, Seungho Cook, Seongjin Yoo, Daeseung Lee, Hongyoon Choi", "custom_title": "IAMSAM: image-based analysis of molecular signatures using the Segment Anything Model", "custom_year": "2024", "font": {"color": "#343434"}, "id": 14, "label": "IAMSAM: image-based analysis of", "shape": "dot", "size": 60}, {"color": "#4d8d8c", "custom_abstract": "Nuclei detection and segmentation in hematoxylin and eosin-stained (H\u0026E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently published Segment Anything Model and a ViT-encoder pre-trained on 104 million histological image patches -achieving state-of-the-art nuclei detection and instance segmentation performance on the PanNuke dataset with a mean panoptic quality of 0.50 and an F 1 -detection score of 0.83. The code is publicly available at https://github.com/TIO-IKIM/CellViT.", "custom_author": "Fabian H\u00f6rst, Moritz Rempe, Lukas Heine, Constantin Seibold, Julius Keyl, Giulia Baldini, Selma Ugurel, Jens Siveke, Barbara Gr\u00fcnwald, Jan Egger, Jens Kleesiek", "custom_title": "CellViT: Vision Transformers for Precise Cell Segmentation and Classification", "custom_year": "2023", "font": {"color": "#343434"}, "id": 15, "label": "CellViT: Vision Transformers for", "shape": "dot", "size": 96}, {"color": "#4d8d8c", "custom_abstract": "We present a rapid, scalable, and high throughput computational pipeline to accurately detect and segment the glomerulus from renal histopathology images with high precision and accuracy. Our proposed method integrates information from fluorescence and bright-field microscopy imaging of renal tissues. For computation, we exploit the simplicity, yet extreme robustness of Butterworth bandpass filter to extract the glomeruli by utilizing the information inherent in the renal tissue stained with immunofluorescence marker sensitive at blue emission wavelength as well as tissue auto-fluorescence. The resulting output is in-turn used to detect and segment multiple glomeruli within the fieldof-view in the same tissue section post-stained with histopathological stains. Our approach, optimized over 40 images, produced a sensitivity/specificity of 0.95/0.84 on n = 66 test images, each containing one or more glomeruli. The work not only has implications in renal histopathology involving diseases with glomerular structural damages, which is vital to track the progression of the disease, but also aids in the development of a tool to rapidly generate a database of glomeruli from whole slide images, essential for training neural networks. The current practice to detect glomerular structural damage is by the manual examination of biopsied renal tissues, which is laborious, time intensive and tedious. Existing automated pipelines employ complex neural networks which are computationally extensive, demand expensive highperformance hardware and require large expert-annotated datasets for training. Our automated method to detect glomerular boundary will aid in rapid extraction of glomerular compartmental features from large renal histopathological images.", "custom_author": "Brandon Ginley, Brendon Lutnick, John Tomaszewski, Pinaki Sarder, Darshana Govind", "custom_title": "Glomerular detection and segmentation from multimodal microscopy images using a Butterworth band-pass filter", "custom_year": "2018", "font": {"color": "#343434"}, "id": 16, "label": "Glomerular detection and segmentation", "shape": "dot", "size": 24}, {"color": "#4d8d8c", "custom_abstract": "Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro-and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct SToCorpus-88M, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.", "custom_author": "Suyuan Zhao, Yizhen Luo, Ganbo Yang, Yan Zhong, Hao Zhou, Zaiqing Nie, Cell Encoder", "custom_title": "SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics", "custom_year": "2025", "font": {"color": "#343434"}, "id": 17, "label": "SToFM: a Multi-scale Foundation", "shape": "dot", "size": 87}, {"color": "#4d8d8c", "custom_abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.", "custom_author": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick", "custom_title": "Segment Anything", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 18, "label": "Segment Anything", "shape": "dot", "size": 84}, {"color": "#4d8d8c", "custom_abstract": "Segmentation algorithms are prone to topological errors on fine-scale structures, e.g., broken connections. We propose a novel method that learns to segment with correct topology. In particular, we design a continuous-valued loss function that enforces a segmentation to have the same topology as the ground truth, i.e., having the same Betti number. The proposed topology-preserving loss function is differentiable and we incorporate it into end-to-end training of a deep neural network. Our method achieves much better performance on the Betti number error, which directly accounts for the topological correctness. It also performs superiorly on other topology-relevant metrics, e.g., the Adjusted Rand Index and the Variation of Information. We illustrate the effectiveness of the proposed method on a broad spectrum of natural and biomedical datasets.", "custom_author": "Xiaoling Hu, Li Fuxin, Dimitris Samaras, Chao Chen", "custom_title": "Topology-Preserving Deep Image Segmentation", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 19, "label": "Topology-Preserving Deep Image Segmentation", "shape": "dot", "size": 81}, {"color": "#4d8d8c", "custom_abstract": "Traditional renal biopsy pathological analysis relies heavily on subjective judgment by pathologists, which is time-consuming and susceptible to inter-observer variability, especially in segmenting glomeruli, renal tubules, and renal vessels. This study aims to develop an efficient and accurate segmentation method for renal pathological tissues to support intelligent diagnostic workflows. By integrating classic models such as Unet, U 2 net, and EfficientNet, along with the state-of-the-art Transformer-based SwinUnet, this research conducts a comprehensive comparison of different architectures in terms of accuracy, speed, and computational resource consumption. Experimental results show that while SwinUnet achieves an F1 score of 84.02 with an inference time of 630s, the proposed EfficientNet-b4+Unet model reaches a higher F1 score of 84.26 with a significantly reduced inference time of 420s, demonstrating its superior efficiency. A complete pipeline is proposed, encompassing data preprocessing, model training, and validation, including multi-scale feature extraction and optimized loss function configurations. Moreover, to enhance segmentation accuracy at tissue boundaries, this study introduces a novel boundary processing strategy, where boundaries are treated as a separate class and assigned higher weights. Experimental validation confirms the effectiveness of this approach. Results on datasets with PAS, PASM, H\u0026E, and Masson staining demonstrate that the improved EfficientNet-b4+Unet model achieves the best balance between performance and speed while exhibiting strong generalization across different staining methods. This study provides a novel technical framework for precise renal pathological tissue segmentation and lays the foundation for future lesion detection and intelligent diagnostic applications.", "custom_author": "Yan Liu, Taiping Wang, Xinxin He, Wei Wang, Qiang He, Juan Jin", "custom_title": "Renal Biopsy Pathological Tissue Segmentation: A Comprehensive Review and Experimental Analysis", "custom_year": "2025", "font": {"color": "#343434"}, "id": 20, "label": "Renal Biopsy Pathological Tissue", "shape": "dot", "size": 84}, {"color": "#4d8d8c", "custom_abstract": "Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.", "custom_author": "Carsen Stringer, Tim Wang, Michalis Michaelos, Marius Pachitariu", "custom_title": "Cellpose: a generalist algorithm for cellular segmentation", "custom_year": "2020", "font": {"color": "#343434"}, "id": 21, "label": "Cellpose: a generalist algorithm", "shape": "dot", "size": 90}, {"color": "#4d8d8c", "custom_abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.", "custom_author": "Olaf Ronneberger, Philipp Fischer, Thomas Brox", "custom_title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "custom_year": "2015", "font": {"color": "#343434"}, "id": 22, "label": "U-Net: Convolutional Networks for", "shape": "dot", "size": 48}, {"color": "#4d8d8c", "custom_abstract": "Probe-based in situ hybridization spatial transcriptomics has emerged as a state-of-the-art for neuroscience research. Accurate segmentation of neurons and non-neuronal cells, a critical step for downstream analysis, remains a big challenge. Using human sensory ganglia neurons as an example, we systematically explore this problem. We evaluated multiple automatic segmentation approaches using quantitative performance metrics and downstream results. We show that careful parameter tuning is essential for achieving accurate segmentation; however, even with optimized parameters, different models still yield distinct classes of errors. To mitigate these errors, we propose a manual quality check, which can validate and refine automated segmentation results. As a future direction, integrating multi-modal imaging data with tailored neural networks may further improve segmentation accuracy. In summary, our results argue that each automated segmentation method has distinct strengths, weaknesses, and characteristic error patterns; and that a manual review is necessary.", "custom_author": "Huasheng Yu, Anna Carroll, Kevin Shen, Nathan Wu, Hanying Yan, Jingwei Xiong, Miguel Mercado, Eric Kaiser, Mayank Gautam, Mingyao Li, Wenqin Luo", "custom_title": "Segmentation Matters: Recognizing the Cell Segmentation Challenge in Spatial Transcriptomics", "custom_year": "2025", "font": {"color": "#343434"}, "id": 23, "label": "Segmentation Matters: Recognizing the", "shape": "dot", "size": 78}, {"color": "#4d8d8c", "custom_abstract": "The detection and segmentation of white blood cells in blood smear images is a key step in medical diagnostics, supporting various downstream tasks such as automated blood cell counting, morphological analysis, cell classification, and disease diagnosis and monitoring. Training robust and accurate models requires large amounts of labeled data, which is both time-consuming and expensive to acquire. In this work, we propose a novel approach for weakly supervised segmentation using neural cellular automata (NCA-WSS). By leveraging the feature maps generated by NCA during classification, we can extract segmentation masks without the need for retraining with segmentation labels. We evaluate our method on three white blood cell microscopy datasets and demonstrate that NCA-WSS significantly outperforms existing weakly supervised approaches. Our work illustrates the potential of NCA for both classification and segmentation in a weakly supervised framework, providing a scalable and efficient solution for medical image analysis.", "custom_author": "Michael Deutges, Chen Yang, Raheleh Salehi, Nassir Navab, Carsten Marr, Ario Sadafi", "custom_title": "Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells", "custom_year": "2025", "font": {"color": "#343434"}, "id": 24, "label": "Neural Cellular Automata for", "shape": "dot", "size": 72}, {"color": "#4d8d8c", "custom_abstract": "However existing works are limited in analyzing the aorta and iliac arteries, above all for vascular anomaly detection and characterization. To close this gap, we propose Dr-SAM, a comprehensive multi-stage framework for vessel segmentation, diameter estimation, and anomaly analysis aiming to examine the peripheral vessels through angiography images. For segmentation we introduce a customized positive/negative point selection mechanism applied on top of the Segment Anything Model (SAM), specifically for medical (Angiography) images. Then we propose a morphological approach to determine the vessel diameters followed by our histogram-driven anomaly detection approach. Moreover, we introduce a new benchmark dataset for the comprehensive analysis of peripheral vessel angiography images which we hope can boost the upcoming research in this direction leading to enhanced diagnostic precision and ultimately better health outcomes for individuals facing vascular issues.", "custom_author": "Vazgen Zohranyan, Vagner Navasardyan, Hayk Navasardyan, Jan Borggrefe, Shant Navasardyan", "custom_title": "Dr-SAM: An End-to-End Framework for Vascular Segmentation, Diameter Estimation, and Anomaly Detection on Angiography Images", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 25, "label": "Dr-SAM: An End-to-End Framework", "shape": "dot", "size": 66}, {"color": "#4d8d8c", "custom_abstract": "Recent progress in image-based spatial RNA profiling enables to spatially resolve tens to hundreds of distinct RNA species with high spatial resolution. It presents new avenues for comprehending tissue organization. In this context, the ability to assign detected RNA transcripts to individual cells is crucial for downstream analyses, such as in-situ cell type calling. Yet, accurate cell segmentation can be challenging in tissue data, in particular in the absence of a high-quality membrane marker. To address this issue, we introduce ComSeg, a segmentation algorithm that operates directly on single RNA positions and that does not come with implicit or explicit priors on cell shape. ComSeg is applicable in complex tissues with arbitrary cell shapes. Through comprehensive evaluations on simulated and experimental datasets, we show that ComSeg outperforms existing state-of-the-art methods for insitu single-cell RNA profiling and in-situ cell type calling. ComSeg is available as a documented and open source pip package at https://github.com/fish-quant/ComSeg.", "custom_author": "Thomas Defard, Hugo Laporte, Mallick Ayan, Juliette Soulier, Sandra Curras-Alonso, Christian Weber, Florian Massip, Jos\u00e9-Arturo Londo\u00f1o-Vallejo, Charles Fouillade, Florian Mueller, Thomas Walter", "custom_title": "A point cloud segmentation framework for image-based spatial transcriptomics", "custom_year": "2024", "font": {"color": "#343434"}, "id": 26, "label": "A point cloud segmentation", "shape": "dot", "size": 78}, {"color": "#4d8d8c", "custom_abstract": "No abstract available.", "custom_author": "Viktor Petukhov, Rosalind Xu, Ruslan Soldatov, Paolo Cadinu, Konstantin Khodosevich, Jeffrey Moffitt, Peter Kharchenko", "custom_title": "Cell segmentation in imaging-based spatial transcriptomics", "custom_year": "2021", "font": {"color": "#343434"}, "id": 27, "label": "Cell segmentation in imaging-based", "shape": "dot", "size": 66}, {"color": "#4d8d8c", "custom_abstract": "Background Image-based machine learning tools hold great promise for clinical applications in pathology research. However, the ideal end-users of these computational tools (e.g., pathologists and biological scientists) often lack the programming experience required for the setup and use of these tools which often rely on the use of command line interfaces. Methods We have developed Histo-Cloud, a tool for segmentation of whole slide images (WSIs) that has an easy-to-use graphical user interface. This tool runs a state-of-the-art convolutional neural network (CNN) for segmentation of WSIs in the cloud and allows the extraction of features from segmented regions for further analysis. Results By segmenting glomeruli, interstitial fibrosis and tubular atrophy, and vascular structures from renal and non-renal WSIs, we demonstrate the scalability, best practices for transfer learning, and effects of dataset variability. Finally, we demonstrate an application for animal model research, analyzing glomerular features in three murine models. Conclusions Histo-Cloud is open source, accessible over the internet, and adaptable for segmentation of any histological structure regardless of stain.", "custom_author": "Brendon Lutnick, David Manthey, Jan Becker, Brandon Ginley, Katharina Moos, Jonathan Zuckerman, Luis Rodrigues, Alexander Gallan, Laura Barisoni, Charles Alpers, Xiaoxin Wang, Komuraiah Myakala, Bryce Jones, Moshe Levi, Jeffrey Kopp, Teruhiko Yoshida, Jarcy Zee, Seung Han, Sanjay Jain, Avi Rosenberg, Kuang Jen, Pinaki Sarder, Brendon Lutnick, Brandon Ginley, Richard Knight, Stewart Lecker, Isaac Stillman, Steve Bogen, Afolarin Amodu, Titlayo Ilori, Insa Schmidt, Shana Maikhor, Laurence Beck, Ashish Verma, Joel Henderson, Ingrid Onul, Sushrut Waikar, Gearoid Mcmahon, Astrid Weins, Mia Colona, M Valerius, Nir Hacohen, Paul Hoover, Anna Greka, Jamie Marshall, Mark Aulisio, Yijiang Chen, Andrew Janowczyk, Catherine Jayapandian, Vidya Viswanathan, William Bush, Dana Crawford, Anant Madabhushi, John O\u2019toole, Emilio Poggio, John Sedor, Leslie Cooperman, Stacey Jolly, Leal Herlitz, Jane Nguyen, Agustin Gonzalez-Vicente, Ellen Palmer, Dianna Sendrey, Jonathan Taliercio, Lakeshia Bush, Kassandra Spates-Harden, Carissa Vinovskis, Petter Bjornstad, Laura Pyle, Paul Appelbaum, Jonathan Barasch, Andrew Bomback, Vivette D\u2019agati, Krzysztof Kiryluk, Karla Mehl, Pietro Canetta, Ning Shang, Olivia Balderes, Satoru Kudose, Theodore Alexandrov, Helmut Rennke, Tarek El-Achkar, Yinghua Cheng, Pierre Dagher, Michael Eadon, Kenneth Dunn, Katherine Kelly, Timothy Sutton, Daria Barwinska, Michael Ferkowicz, Seth Winfree, Sharon Bledsoe, Marcelino Rivera, James Williams, Ricardo Ferreira, Katy Borner, Andreas Bueckle, Bruce Herr, Ellen Quardokus, Elizabeth Record, Jing Su, Debora Gisch, Stephanie Wofford, Yashvardhan Jain, Chirag Parikh, Celia Corona-Villalobos, Steven Menez, Yumeng Wen, Camille Johansen, Sylvia Rosas, Neil Roy, Mark Williams, Jennifer Sun, Joseph Ardayfio, Jack Bebiak, Keith Brown, Catherine Campbell, John Saul, Anna Shpigel, Christy Stutzke, Robert Koewler, Taneisha Campbell, Lynda Hayashi, Nichole Jefferson, Glenda Roberts, Roy Pinkeney, Evren Azeloglu, Cijang He, Ravi Iyengar, Jens Hansen, Yuguang Xiong, Pottumarthi Prasad, Anand Srivastava, Brad Rovin, Samir Parikh, John Shapiro, Sethu Madhavan, Christopher Anderton, Ljiljana Pasa-Tolic, Dusan Velickovic, Jessica Lukowski, George Oliver, Olga Troyanskaya, Rachel Sealfon, Aaron Wong, Katherine Tuttle, Ari Pollack, Yury Goltsev, Kun Zhang, Blue Lake, Zoltan Laszik, Garry Nolan, Patrick Boada, Minnie Sarwal, Kavya Anjani, Tara Sigdel, Tariq Mukatash, Paul Lee, Rita Alloway, E Woodle, Ashley Burg, Adele Rike, Tiffany Shi, Heather Ascani, Ulysses Balis, Jeffrey Hodgin, Matthias Kretzler, Chrysta Lienczewski, Laura Mariani, Rajasree Menon, Becky Steck, Yougqun He, Edgar Otto, Jennifer Schaub, Victoria Blanc, Sean Eddy, Ninive Conser, Jinghui Luo, Renee Frey, Paul Palevsky, Matthew Rosengart, John Kellum, Daniel Hall, Parmjeet Randhawa, Mitchell Tublin, Raghavan Murugan, Michele Elder, James Winters, Tina Vita, Filitsa Bender, Roderick Tan, Matthew Gilliam, Kristina Blank, Jonas Carson, Ian De Boer, Ashveena Dighe, Jonathan Himmelfarb, Sean Mooney, Stuart Shankland, Kayleen Williams, Christopher Park, Frederick Dowd, Robyn Mcclelland, Stephen Daniel, Andrew Hoofnagle, Adam Wilcox, Stephanie Grewenow, Ashley Berglund, Christine Limonte, Kasra Rezaei, Ruikang Wang, Jamie Snyder, Brooke Berry, Yunbi Nam, Natalya Sarkisova, Shweta Bansal, Kumar Sharma, Manjeri Venkatachalam, Guanshi Zhang, Annapurna Pamreddy, Hongping Ye, Richard Montellano, Robert Toto, Miguel Vazquez, Simon Lee, R Miller, Orson Moe, Jose Torrealba, Nancy Wang, Asra Kermani, Kamalanathan Sambandam, Harold Park, S Hedayati, Christopher Lu, Natasha Wen, Jiten Patel, Anil Pillai, Dianbo Zhang, Mujeeb Basit, Allen Hendricks, Richard Caprioli, Nathan Patterson, Kavya Sharman, Jeffrey Spraggins, Raf Van De Plas, Anitha Vijayan, Joseph Gaut, Jeanine Basta, Sabine Diettman, Michael Rauchman, Dennis Moledina, Francis Wilson, Ugochukwu Ugwuowo, Tanima Arora, Melissa Shaw, Lloyd Cantley, Vijaykumar Kakade, Angela Victoria-Castro", "custom_title": "A user-friendly tool for cloud-based whole slide image segmentation with examples from renal histopathology", "custom_year": "2022", "font": {"color": "#343434"}, "id": 28, "label": "A user-friendly tool for", "shape": "dot", "size": 81}, {"color": "#4d8d8c", "custom_abstract": "Glomerular histomorphology establishes kidney disease diagnosis and prognosis. Spatial transcriptomics facilitates spatial resolution of molecular signatures superimposed upon histology. We trained a machine learning (ML) method to automatically segment glomeruli and quantify pixel level image features to align with spatial transcriptomics (ST) performed on the same section.", "custom_author": "Unknown", "custom_title": "Computational Segmentation of Glomeruli to Align Histomorphology With Spatial Transcriptomic Signature Session Information", "custom_year": "N/A", "font": {"color": "#343434"}, "id": 29, "label": "Computational Segmentation of Glomeruli", "shape": "dot", "size": 15}]);
                  edges = new vis.DataSet([{"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 1, "value": 7}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 2, "value": 3}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 4, "value": 14}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 5, "value": 5}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 6, "value": 3}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 7, "value": 4}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 8, "value": 4}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 9, "value": 1}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 10, "value": 5}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 11, "value": 2}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 12, "value": 2}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 13, "value": 9}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 15, "value": 13}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 17, "value": 2}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 18, "value": 1}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 19, "value": 2}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 21, "value": 6}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 23, "value": 4}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 24, "value": 2}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 26, "value": 9}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 27, "value": 8}, {"color": "#cbd5e0", "from": 0, "opacity": 0.4, "to": 28, "value": 8}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 2, "value": 2}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 3, "value": 1}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 4, "value": 3}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 5, "value": 2}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 6, "value": 8}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 7, "value": 3}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 8, "value": 8}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 9, "value": 1}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 10, "value": 2}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 11, "value": 5}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 12, "value": 10}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 13, "value": 14}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 15, "value": 20}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 17, "value": 5}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 18, "value": 5}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 19, "value": 9}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 21, "value": 8}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 23, "value": 5}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 24, "value": 7}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 25, "value": 4}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 26, "value": 1}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 27, "value": 2}, {"color": "#cbd5e0", "from": 1, "opacity": 0.4, "to": 28, "value": 2}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 4, "value": 9}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 5, "value": 4}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 6, "value": 1}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 7, "value": 1}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 10, "value": 11}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 11, "value": 2}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 13, "value": 6}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 14, "value": 9}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 15, "value": 2}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 17, "value": 8}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 19, "value": 1}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 21, "value": 7}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 23, "value": 8}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 26, "value": 16}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 27, "value": 13}, {"color": "#cbd5e0", "from": 2, "opacity": 0.4, "to": 28, "value": 4}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 5, "value": 3}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 6, "value": 1}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 7, "value": 4}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 9, "value": 1}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 10, "value": 8}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 11, "value": 1}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 13, "value": 14}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 15, "value": 10}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 17, "value": 4}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 18, "value": 3}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 20, "value": 2}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 21, "value": 17}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 23, "value": 7}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 26, "value": 9}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 27, "value": 13}, {"color": "#cbd5e0", "from": 4, "opacity": 0.4, "to": 28, "value": 5}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 7, "value": 1}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 9, "value": 1}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 10, "value": 5}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 13, "value": 5}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 14, "value": 2}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 15, "value": 1}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 17, "value": 2}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 18, "value": 1}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 21, "value": 3}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 23, "value": 4}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 26, "value": 10}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 27, "value": 6}, {"color": "#cbd5e0", "from": 5, "opacity": 0.4, "to": 28, "value": 1}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 7, "value": 6}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 8, "value": 6}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 10, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 11, "value": 2}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 12, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 13, "value": 8}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 14, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 15, "value": 10}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 17, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 18, "value": 3}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 19, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 21, "value": 2}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 23, "value": 1}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 24, "value": 4}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 26, "value": 2}, {"color": "#cbd5e0", "from": 6, "opacity": 0.4, "to": 27, "value": 1}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 3, "value": 1}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 8, "value": 3}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 10, "value": 6}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 11, "value": 3}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 12, "value": 5}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 13, "value": 9}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 14, "value": 3}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 15, "value": 9}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 17, "value": 2}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 18, "value": 6}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 19, "value": 3}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 21, "value": 4}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 23, "value": 2}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 24, "value": 2}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 26, "value": 2}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 27, "value": 2}, {"color": "#cbd5e0", "from": 7, "opacity": 0.4, "to": 28, "value": 2}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 3, "value": 4}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 9, "value": 2}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 10, "value": 2}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 11, "value": 6}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 12, "value": 5}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 13, "value": 7}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 15, "value": 9}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 17, "value": 2}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 18, "value": 4}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 19, "value": 13}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 20, "value": 5}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 21, "value": 2}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 23, "value": 1}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 24, "value": 8}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 8, "opacity": 0.4, "to": 28, "value": 3}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 3, "value": 10}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 10, "value": 2}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 11, "value": 5}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 12, "value": 4}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 13, "value": 2}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 15, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 18, "value": 2}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 19, "value": 4}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 21, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 24, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 25, "value": 2}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 26, "value": 1}, {"color": "#cbd5e0", "from": 9, "opacity": 0.4, "to": 28, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 11, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 12, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 13, "value": 4}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 14, "value": 3}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 15, "value": 4}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 17, "value": 7}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 18, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 19, "value": 2}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 20, "value": 3}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 21, "value": 7}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 23, "value": 5}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 24, "value": 1}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 26, "value": 13}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 27, "value": 8}, {"color": "#cbd5e0", "from": 10, "opacity": 0.4, "to": 28, "value": 6}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 3, "value": 3}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 12, "value": 8}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 13, "value": 4}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 15, "value": 9}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 17, "value": 4}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 18, "value": 8}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 19, "value": 18}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 20, "value": 3}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 21, "value": 3}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 23, "value": 1}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 24, "value": 5}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 25, "value": 4}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 26, "value": 1}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 27, "value": 1}, {"color": "#cbd5e0", "from": 11, "opacity": 0.4, "to": 28, "value": 2}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 3, "value": 3}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 13, "value": 6}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 15, "value": 10}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 17, "value": 2}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 18, "value": 4}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 19, "value": 10}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 20, "value": 4}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 21, "value": 2}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 23, "value": 1}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 24, "value": 4}, {"color": "#cbd5e0", "from": 12, "opacity": 0.4, "to": 25, "value": 5}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 3, "value": 1}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 14, "value": 2}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 15, "value": 18}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 17, "value": 12}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 18, "value": 6}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 19, "value": 5}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 21, "value": 19}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 23, "value": 8}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 24, "value": 5}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 26, "value": 6}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 27, "value": 4}, {"color": "#cbd5e0", "from": 13, "opacity": 0.4, "to": 28, "value": 1}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 3, "value": 2}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 16, "value": 1}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 17, "value": 6}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 18, "value": 6}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 19, "value": 9}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 20, "value": 7}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 21, "value": 11}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 23, "value": 4}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 24, "value": 7}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 25, "value": 2}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 26, "value": 2}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 27, "value": 2}, {"color": "#cbd5e0", "from": 15, "opacity": 0.4, "to": 28, "value": 2}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 18, "value": 1}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 19, "value": 1}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 20, "value": 1}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 21, "value": 5}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 23, "value": 10}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 24, "value": 4}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 25, "value": 2}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 26, "value": 7}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 27, "value": 4}, {"color": "#cbd5e0", "from": 17, "opacity": 0.4, "to": 28, "value": 2}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 3, "value": 3}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 14, "value": 4}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 19, "value": 3}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 20, "value": 2}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 21, "value": 4}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 23, "value": 1}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 24, "value": 1}, {"color": "#cbd5e0", "from": 18, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 3, "value": 5}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 20, "value": 5}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 21, "value": 3}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 22, "value": 2}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 24, "value": 6}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 26, "value": 1}, {"color": "#cbd5e0", "from": 19, "opacity": 0.4, "to": 28, "value": 3}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 3, "value": 2}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 21, "value": 4}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 22, "value": 2}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 24, "value": 1}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 25, "value": 2}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 26, "value": 2}, {"color": "#cbd5e0", "from": 20, "opacity": 0.4, "to": 28, "value": 13}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 23, "value": 3}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 24, "value": 2}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 26, "value": 6}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 27, "value": 6}, {"color": "#cbd5e0", "from": 21, "opacity": 0.4, "to": 28, "value": 4}, {"color": "#cbd5e0", "from": 23, "opacity": 0.4, "to": 14, "value": 1}, {"color": "#cbd5e0", "from": 23, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 23, "opacity": 0.4, "to": 26, "value": 8}, {"color": "#cbd5e0", "from": 23, "opacity": 0.4, "to": 27, "value": 5}, {"color": "#cbd5e0", "from": 23, "opacity": 0.4, "to": 28, "value": 3}, {"color": "#cbd5e0", "from": 24, "opacity": 0.4, "to": 3, "value": 2}, {"color": "#cbd5e0", "from": 24, "opacity": 0.4, "to": 25, "value": 1}, {"color": "#cbd5e0", "from": 24, "opacity": 0.4, "to": 28, "value": 1}, {"color": "#cbd5e0", "from": 25, "opacity": 0.4, "to": 3, "value": 1}, {"color": "#cbd5e0", "from": 26, "opacity": 0.4, "to": 14, "value": 6}, {"color": "#cbd5e0", "from": 26, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 26, "opacity": 0.4, "to": 27, "value": 16}, {"color": "#cbd5e0", "from": 26, "opacity": 0.4, "to": 28, "value": 5}, {"color": "#cbd5e0", "from": 27, "opacity": 0.4, "to": 14, "value": 2}, {"color": "#cbd5e0", "from": 27, "opacity": 0.4, "to": 28, "value": 4}, {"color": "#cbd5e0", "from": 28, "opacity": 0.4, "to": 3, "value": 1}, {"color": "#cbd5e0", "from": 28, "opacity": 0.4, "to": 22, "value": 1}, {"color": "#cbd5e0", "from": 3, "opacity": 0.4, "to": 16, "value": 1}, {"color": "#cbd5e0", "from": 3, "opacity": 0.4, "to": 22, "value": 3}, {"color": "#cbd5e0", "from": 22, "opacity": 0.4, "to": 16, "value": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"physics": {"forceAtlas2Based": {"gravitationalConstant": -150, "centralGravity": 0.005, "springLength": 200, "avoidOverlap": 1}, "solver": "forceAtlas2Based", "stabilization": {"enabled": true, "iterations": 1000, "updateInterval": 25, "onlyDynamicEdges": false, "fit": true}}, "interaction": {"hover": true, "dragNodes": true, "hideEdgesOnDrag": false, "hideNodesOnDrag": false}, "nodes": {"font": {"size": 40}}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>
    <div id="left-sidebar" style="position:fixed; left:0; top:0; width:300px; height:100%; background:#f8f9fa; border-right:1px solid #ddd; padding:20px; overflow-y:auto; z-index:1000; font-family: sans-serif;">
        <h3 style="font-size: 1.1em; margin-bottom: 15px; color: #333;">All Papers (30)</h3>
        <div id="paper-list">
            <div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Evaluating cell AI foundation models in kidney pathology with human-in-the-loop enrichment</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>WEAK-TO-STRONG GENERALIZATION ENABLES FULLY AUTOMATED DE NOVO TRAINING OF MULTI-HEAD MASK-RCNN MODEL FOR SEGMENTING DENSELY OVERLAPPING CELL NUCLEI IN MULTIPLEX WHOLE-SLICE BRAIN IMAGES</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Computational Approaches and Challenges in Spatial Transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>DeepFlux for Skeletons in the Wild</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Using transcripts to refine image based cell segmentation with FastReseg</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Adaptive Segment Anything Model for Spatial Transcriptomic Cell Segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Segment anything in medical images</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>SAM-Adapter: Adapting Segment Anything in Underperformed Scenes</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Deep Distance Transform for Tubular Structure Segmentation in CT Scans</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>A visualomics foundation model to bridge histopathology with spatial transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>clDice -a Novel Topology-Preserving Loss Function for Tubular Structure Segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>UCS: Universal Model for Curvilinear Structure Segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Cellpose-SAM: superhuman generalization for cellular segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>IAMSAM: image-based analysis of molecular signatures using the Segment Anything Model</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>CellViT: Vision Transformers for Precise Cell Segmentation and Classification</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Glomerular detection and segmentation from multimodal microscopy images using a Butterworth band-pass filter</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Segment Anything</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Topology-Preserving Deep Image Segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Renal Biopsy Pathological Tissue Segmentation: A Comprehensive Review and Experimental Analysis</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Cellpose: a generalist algorithm for cellular segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>U-Net: Convolutional Networks for Biomedical Image Segmentation</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Segmentation Matters: Recognizing the Cell Segmentation Challenge in Spatial Transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Dr-SAM: An End-to-End Framework for Vascular Segmentation, Diameter Estimation, and Anomaly Detection on Angiography Images</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>A point cloud segmentation framework for image-based spatial transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Cell segmentation in imaging-based spatial transcriptomics</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>A user-friendly tool for cloud-based whole slide image segmentation with examples from renal histopathology</div><div style='padding:10px; border-bottom:1px solid #eee; font-size:0.9em; color:#444;'>Computational Segmentation of Glomeruli to Align Histomorphology With Spatial Transcriptomic Signature Session Information</div>
        </div>
    </div>

    <div id="right-sidebar" style="position:fixed; right:0; top:0; width:400px; height:100%; background:white; border-left:1px solid #ddd; padding:25px; overflow-y:auto; z-index:1000; font-family: sans-serif; box-shadow: -2px 0 10px rgba(0,0,0,0.05);">
        <h2 id="s_title" style="font-size: 1.2em; color: #333; margin-bottom:10px;">Hover over a node</h2>
        <div style="margin-bottom: 15px;">
            <p style="color: #333; font-weight: bold; margin-bottom: 5px;">Authors:</p>
            <p id="s_author" style="color: #666; font-size: 0.95em; line-height: 1.4;">-</p>
        </div>
        <p style="color: #666; font-size: 0.95em;"><strong style="color: #333;">Year:</strong> <span id="s_year">-</span></p>
        <hr style="border: 0; border-top: 1px solid #eee; margin: 20px 0;">
        <p id="s_abstract" style="font-size: 1em; line-height: 1.6; color: #444; white-space: pre-wrap;">Abstract will appear here.</p>
    </div>

    <script>
        // Use double braces { } so Python f-string doesn't crash
        network.on("stabilizationIterationsDone", function () {
            network.setOptions({ physics: false });
        });
        
        network.on("hoverNode", function (params) {
            var nodeData = nodes.get(params.node);
            document.getElementById("s_title").innerText = nodeData.custom_title;
            document.getElementById("s_author").innerText = nodeData.custom_author;
            document.getElementById("s_year").innerText = nodeData.custom_year;
            document.getElementById("s_abstract").innerText = nodeData.custom_abstract;
        });
    </script>
    