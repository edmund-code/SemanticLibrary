<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Segment Anything</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<title level="a" type="main">Segment Anything</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BAD1302D43BD908F4A8B81B3B1F0FC5B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2026-01-06T01:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization <ref type="bibr" target="#b9">[10]</ref>. These "foundation models" <ref type="bibr" target="#b7">[8]</ref> can generalize to tasks and data distributions beyond those seen during training. This capability is often implemented with prompt engineering in which hand-crafted text is used to prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with abundant text corpora from the web, these models' zero and few-shot performance compares surprisingly well to (even matching in some cases) fine-tuned models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. Empirical trends show this behavior improving with model scale, dataset size, and total training compute <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Foundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP <ref type="bibr" target="#b79">[80]</ref> and ALIGN <ref type="bibr" target="#b52">[53]</ref> use contrastive learning to train text and image encoders that align the two modalities. Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions. Such encoders also compose effectively with other modules to enable downstream tasks, such as image generation (e.g., DALL•E <ref type="bibr" target="#b80">[81]</ref>). While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist.</p><p>In this work, our goal is to build a foundation model for image segmentation. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task that enables powerful generalization. With this model, we aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering.</p><p>The success of this plan hinges on three components: task, model, and data. To develop them, we address the following questions about image segmentation:</p><p>1. What task will enable zero-shot generalization? 2. What is the corresponding model architecture? 3. What data can power this task and model?</p><p>This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.</p><p>Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. These questions are entangled and require a comprehensive solution. We start by defining a promptable segmentation task that is general enough to provide a powerful pretraining objective and to enable a wide range of downstream applications. This task requires a model that supports flexible prompting and can output segmentation masks in realtime when prompted to allow for interactive use. To train our model, we need a diverse, large-scale source of data. Unfortunately, there is no web-scale data source for segmentation; to address this, we build a "data engine", i.e., we iterate between using our efficient model to assist in data collection and using the newly collected data to improve the model. We introduce each interconnected component next, followed by the dataset we created and the experiments that demonstrate the effectiveness of our approach.</p><p>Task ( §2). In NLP and more recently computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using "prompting" techniques. Inspired by this line of work, we propose the promptable segmentation task, where the goal is to return a valid segmentation mask given any segmentation prompt (see Fig. <ref type="figure">1a</ref>). A prompt simply specifies what to segment in an image, e.g., a prompt can include spatial or text information identifying an object. The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering.</p><p>Model ( §3). The promptable segmentation task and the goal of real-world use impose constraints on the model architecture. In particular, the model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use, and must be ambiguity-aware. Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks. We refer to this model as the Segment Anything Model, or SAM (see Fig. <ref type="figure">1b</ref>). By separating SAM into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts. Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in ⇠50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make SAM ambiguity-aware, we design it to predict multiple masks for a single prompt allowing SAM to naturally handle ambiguity, such as the shirt vs. person example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data engine ( §4).</head><p>To achieve strong generalization to new data distributions, we found it necessary to train SAM on a large and diverse set of masks, beyond any segmentation dataset that already exists. While a typical approach for foundation models is to obtain data online <ref type="bibr" target="#b79">[80]</ref>, masks are not naturally abundant and thus we need an alternative strategy. Our solution is to build a "data engine", i.e., we co-develop our model with model-in-the-loop dataset annotation (see Fig. <ref type="figure">1c</ref>). Our data engine has three stages: assisted-manual, semi-automatic, and fully automatic. In the first stage, SAM assists annotators in annotating masks, similar to a classic interactive segmentation setup. In the second stage, SAM can automatically generate masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects, helping increase mask diversity. In the final stage, we prompt SAM with a regular grid of foreground points, yielding on average ⇠100 high-quality masks per image.</p><p>Dataset ( §5). Our final dataset, SA-1B, includes more than 1B masks from 11M licensed and privacy-preserving images (see Fig. <ref type="figure">2</ref>). SA-1B, collected fully automatically using the final stage of our data engine, has 400⇥ more masks than any existing segmentation dataset <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b113">115,</ref><ref type="bibr" target="#b57">58]</ref>, and as we verify extensively, the masks are of high quality and diversity. Beyond its use in training SAM to be robust and general, we hope SA-1B becomes a valuable resource for research aiming to build new foundation models.</p><p>Experiments ( §6). We extensively evaluate SAM. First, using a diverse new suite of 23 segmentation datasets, we find that SAM produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction. These results suggest that SAM can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond SAM's training data. Nevertheless, room for improvement remains, as we discuss in §7.</p><p>Responsible AI. We provide model/dataset cards and report on potential fairness concerns and biases when using SA-1B and SAM in the supplement. Images in SA-1B span a geographically and economically diverse set of regions and we found that SAM performs similarly across different groups of people. Together, we hope this will make our work more equitable for real-world use cases.</p><p>Release. We are releasing the SA-1B dataset for research purposes and making SAM available under a permissive open license (Apache 2.0) at <ref type="url" target="https://segment-anything.com">https://segment-anything.com</ref>. We also showcase SAM's capabilities with an online demo. Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are ⇠100 masks per image on average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Segment Anything Task</head><p>We take inspiration from NLP, where the next token prediction task is used for foundation model pre-training and to solve diverse downstream tasks via prompt engineering <ref type="bibr" target="#b9">[10]</ref>. To build a foundation model for segmentation, we aim to define a task with analogous capabilities.</p><p>Task. We start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground / background points, a rough box or mask, free-form text, or, in general, any information indicating what to segment in an image. The promptable segmentation task, then, is to return a valid segmentation mask given any prompt. The requirement of a "valid" mask simply means that even when a prompt is ambiguous and could refer to multiple objects (e.g., recall the shirt vs. person example, and see Fig. <ref type="figure" target="#fig_1">3</ref>), the output should be a reasonable mask for at least one of those objects. This requirement is similar to expecting a language model to output a coherent response to an ambiguous prompt. We choose this task because it leads to a natural pre-training algorithm and a general method for zero-shot transfer to downstream segmentation tasks via prompting.</p><p>Pre-training. The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (e.g., points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth. We adapt this method from interactive segmentation <ref type="bibr" target="#b105">[107,</ref><ref type="bibr" target="#b67">68]</ref>, although unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, our aim is to always predict a valid mask for any prompt even when the prompt is ambiguous. This ensures that a pre-trained model is effective in use cases that involve ambiguity, including automatic annotation as required by our data engine §4. We note that performing well at this task is challenging and requires specialized modeling and training loss choices, which we discuss in §3.</p><p>Zero-shot transfer. Intuitively, our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts. For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector's box output as a prompt to our model. In general, a wide array of practical segmentation tasks can be cast as prompting. In addition to automatic dataset labeling, we explore five diverse example tasks in our experiments in §6.</p><p>Related tasks. Segmentation is a broad field: there's interactive segmentation <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b105">107]</ref>, edge detection <ref type="bibr" target="#b2">[3]</ref>, super pixelization <ref type="bibr" target="#b82">[83]</ref>, object proposal generation <ref type="bibr" target="#b1">[2]</ref>, foreground segmentation <ref type="bibr" target="#b91">[92]</ref>, semantic segmentation <ref type="bibr" target="#b87">[88]</ref>, instance segmentation <ref type="bibr" target="#b63">[64]</ref>, panoptic segmentation <ref type="bibr" target="#b56">[57]</ref>, etc. The goal of our promptable segmentation task is to produce a broadly capable model that can adapt to many (though not all) existing and new segmentation tasks via prompt engineering. This capability is a form of task generalization <ref type="bibr" target="#b24">[25]</ref>. Note that this is different than previous work on multi-task segmentation systems. In a multi-task system, a single model performs a fixed set of tasks, e.g., joint semantic, instance, and panoptic segmentation <ref type="bibr" target="#b110">[112,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">52]</ref>, but the training and test tasks are the same. An important distinction in our work is that a model trained for promptable segmentation can perform a new, different task at inference time by acting as a component in a larger system, e.g., to perform instance segmentation, a promptable segmentation model is combined with an existing object detector.</p><p>Discussion. Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. This approach is analogous to how other foundation models are used, e.g., how CLIP <ref type="bibr" target="#b79">[80]</ref> is the text-image alignment component of the DALL•E <ref type="bibr" target="#b80">[81]</ref> image generation system. We anticipate that composable system design, powered by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks. It's also interesting to compare promptable and interactive segmentation through the lens of composition: while interactive segmentation models are designed with human users in mind, a model trained for promptable segmentation can also be composed into a larger algorithmic system as we will demonstrate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Segment Anything Model</head><p>We next describe the Segment Anything Model (SAM) for promptable segmentation. SAM has three components, illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>: an image encoder, a flexible prompt encoder, and a fast mask decoder. We build on Transformer vision models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b59">60]</ref> with specific tradeoffs for (amortized) real-time performance. We describe these components at a high-level here, with details in §B.</p><p>Image encoder. Motivated by scalability and powerful pretraining methods, we use an MAE <ref type="bibr" target="#b45">[46]</ref> pre-trained Vision Transformer (ViT) <ref type="bibr" target="#b31">[32]</ref> minimally adapted to process high resolution inputs <ref type="bibr" target="#b59">[60]</ref>. The image encoder runs once per image and can be applied prior to prompting the model. Prompt encoder. We consider two sets of prompts: sparse (points, boxes, text) and dense (masks). We represent points and boxes by positional encodings <ref type="bibr" target="#b92">[93]</ref> summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP <ref type="bibr" target="#b79">[80]</ref>. Dense prompts (i.e., masks) are embedded using convolutions and summed element-wise with the image embedding.</p><p>Mask decoder. The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask. This design, inspired by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, employs a modification of a Transformer decoder block <ref type="bibr" target="#b99">[101]</ref> followed by a dynamic mask prediction head. Our modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embeddings. After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location.</p><p>Resolving ambiguity. With one output, the model will average multiple valid masks if given an ambiguous prompt.</p><p>To address this, we modify the model to predict multiple output masks for a single prompt (see Fig. <ref type="figure" target="#fig_1">3</ref>). We found 3 mask outputs is sufficient to address most common cases (nested masks are often at most three deep: whole, part, and subpart). During training, we backprop only the minimum loss <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62]</ref> over masks. To rank masks, the model predicts a confidence score (i.e., estimated IoU) for each mask.</p><p>Efficiency. The overall model design is largely motivated by efficiency. Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in ⇠50ms. This runtime performance enables seamless, real-time interactive prompting of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Losses and training.</head><p>We supervise mask prediction with the linear combination of focal loss <ref type="bibr" target="#b62">[63]</ref> and dice loss <ref type="bibr" target="#b70">[71]</ref> used in <ref type="bibr" target="#b12">[13]</ref>. We train for the promptable segmentation task using a mixture of geometric prompts (for text prompts see §6.2). Following <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b35">36]</ref>, we simulate an interactive setup by randomly sampling prompts in 11 rounds per mask, allowing SAM to integrate seamlessly into our data engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Segment Anything Data Engine</head><p>As segmentation masks are not abundant on the internet, we built a data engine to enable the collection of our 1.1B mask dataset, SA-1B. The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. We go into details of each next.</p><p>Assisted-manual stage. In the first stage, resembling classic interactive segmentation, a team of professional annotators labeled masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by SAM. Masks could be refined using pixelprecise "brush" and "eraser" tools. Our model-assisted annotation runs in real-time directly inside a browser (using precomputed image embeddings) enabling a truly interactive experience. We did not impose semantic constraints for labeling objects, and annotators freely labeled both "stuff" and "things" <ref type="bibr" target="#b0">[1]</ref>. We suggested annotators label objects they could name or describe, but did not collect these names or descriptions. Annotators were asked to label objects in order of prominence and were encouraged to proceed to the next image once a mask took over 30 seconds to annotate.</p><p>At the start of this stage, SAM was trained using common public segmentation datasets. After sufficient data annotation, SAM was retrained using only newly annotated masks. As more masks were collected, the image encoder was scaled from ViT-B to ViT-H and other architectural details evolved; in total we retrained our model 6 times. Average annotation time per mask decreased from 34 to 14 seconds as the model improved. We note that 14 seconds is 6.5⇥ faster than mask annotation for COCO <ref type="bibr" target="#b63">[64]</ref> and only 2⇥ slower than bounding-box labeling with extreme points <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b68">69]</ref>. As SAM improved, the average number of masks per image increased from 20 to 44 masks. Overall, we collected 4.3M masks from 120k images in this stage.</p><p>Semi-automatic stage. In this stage, we aimed to increase the diversity of masks in order to improve our model's ability to segment anything. To focus annotators on less prominent objects, we first automatically detected confident masks. Then we presented annotators with images prefilled with these masks and asked them to annotate any additional unannotated objects. To detect confident masks, we trained a bounding box detector <ref type="bibr" target="#b81">[82]</ref> on all first stage masks using a generic "object" category. During this stage we collected an additional 5.9M masks in 180k images (for a total of 10.2M masks). As in the first stage, we periodically retrained our model on newly collected data (5 times). Average annotation time per mask went back up to 34 seconds (excluding the automatic masks) as these objects were more challenging to label. The average number of masks per image went from 44 to 72 masks (including the automatic masks).</p><p>Fully automatic stage. In the final stage, annotation was fully automatic. This was feasible due to two major enhancements to our model. First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage. Second, by this stage we had developed the ambiguity-aware model, which allowed us to predict valid masks even in ambiguous cases. Specifically, we prompted the model with a 32⇥32 regular grid of points and for each point predicted a set of masks that may correspond to valid objects. With the ambiguity-aware model, if a point lies on a part or subpart, our model will return the subpart, part, and whole object. The IoU prediction module of our model is used to select confident masks; moreover, we identified and selected only stable masks (we consider a mask stable if thresholding the probability map at 0.5 and 0.5 + results in similar masks). Finally, after selecting the confident and stable masks, we applied non-maximal suppression (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see §C. We applied fully automatic mask generation to all 11M images in our dataset, producing a total of 1.1B high-quality masks. We describe and analyze the resulting dataset, SA-1B, next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Segment Anything Dataset</head><p>Our dataset, SA-1B, consists of 11M diverse, highresolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks collected with our data engine. We compare SA-1B with existing datasets and analyze mask quality and properties. We are releasing SA-1B to aid future development of foundation models for computer vision. We note that SA-1B will be released under a favorable license agreement for certain research uses and with protections for researchers.</p><p>Images. We licensed a new set of 11M images from a provider that works directly with photographers. These images are high resolution (3300⇥4950 pixels on average), and the resulting data size can present accessibility and storage challenges. Therefore, we are releasing downsampled images with their shortest side set to 1500 pixels. Even after downsampling, our images are significantly higher resolution than many existing vision datasets (e.g., COCO <ref type="bibr" target="#b63">[64]</ref> images are ⇠480⇥640 pixels). Note that most models today operate on much lower resolution inputs. Faces and vehicle license plates have been blurred in the released images.</p><p>Masks. Our data engine produced 1.1B masks, 99.1% of which were generated fully automatically. Therefore, the quality of the automatic masks is centrally important. We compare them directly to professional annotations and look at how various mask properties compare to prominent segmentation datasets. Our main conclusion, as borne out in the analysis below and the experiments in §6, is that our automatic masks are high quality and effective for training models. Motivated by these findings, SA-1B only includes automatically generated masks.</p><p>Mask quality. To estimate mask quality, we randomly sampled 500 images (⇠50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise "brush" and "eraser" editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94% of pairs have greater than 90% IoU (and 97% of pairs have greater than 75% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91% IoU <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b57">58]</ref>. Our experiments in §6 confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine.</p><p>Figure <ref type="figure">6</ref>: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B has 11⇥ more images and 400⇥ more masks than the largest existing segmentation dataset Open Images <ref type="bibr" target="#b57">[58]</ref>.</p><p>Mask properties. In Fig. <ref type="figure" target="#fig_3">5</ref> we plot the spatial distribution of object centers in SA-1B compared to the largest existing segmentation datasets. Common photographer biases are present in all datasets. We observe that SA-1B has greater coverage of image corners compared to LVIS v1 <ref type="bibr" target="#b42">[43]</ref> and ADE20K <ref type="bibr" target="#b113">[115]</ref>, the two most similarly distributed datasets, while COCO <ref type="bibr" target="#b63">[64]</ref> and Open Images V5 <ref type="bibr" target="#b57">[58]</ref> have a more prominent center bias. In Fig. <ref type="figure">6</ref> (legend) we compare these datasets by size. SA-1B has 11⇥ more images and 400⇥ more masks than the second largest, Open Images. On average, it has 36⇥ more masks per image than Open Images. The closest dataset in this respect, ADE20K, still has 3.5⇥ fewer masks per image. Fig. <ref type="figure">6</ref> (left) plots the masks-perimage distribution. Next, we look at image-relative mask size (square root of the mask area divided by image area) in Fig. <ref type="figure">6</ref> (middle). As expected, since our dataset has more masks per image, it also tends to include a greater percentage of small and medium relative-size masks. Finally, to analyze shape complexity, we look at mask concavity (1 minus mask area divided by area of mask's convex hull) in Fig. <ref type="figure">6</ref> (right). Since shape complexity is correlated with mask size, we control for the datasets' mask size distributions by first performing stratified sampling from binned mask sizes. We observe that the concavity distribution of our masks is broadly similar to that of other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Zero-Shot Transfer Experiments</head><p>In this section, we present zero-shot transfer experiments with SAM, the Segment Anything Model. We consider five tasks, four of which differ significantly from the promptable segmentation task used to train SAM. These experiments evaluate SAM on datasets and tasks that were not seen during training (our usage of "zero-shot transfer" follows its usage in CLIP <ref type="bibr" target="#b79">[80]</ref>). The datasets may include novel image distributions, such as underwater or ego-centric images that, to our knowledge, do not appear in SA-1B.</p><p>Our experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt. We emphasize the challenging scenario of a single foreground point prompt, since it is more likely to be ambiguous than other more specific prompts. Next, we present a sequence of experiments that traverse low, mid, and high-level image understanding and roughly parallel the historical development of the field. Specifically, we prompt SAM to (1) perform edge detection, (2) segment everything, i.e. object proposal generation, (3) segment detected objects, i.e. instance segmentation, and (4), as a proof-of-concept, to segment objects from free-form text. These four tasks differ significantly from the promptable segmentation task that SAM was trained on and are implemented via prompt engineering. We report zero-shot single point valid mask evaluation and zero-shot text to mask proof-of-concept in the main text. We refer readers to the supplement for our experiments with zero-shot edge detection, object proposal, and instance segmentation. In addition, we report a set of ablations in the supplement. We analyze SAM performance with respect to the size and composition of its training data as well as the image encoder architecture.</p><p>Implementation. Unless otherwise specified: (1) SAM uses an MAE <ref type="bibr" target="#b45">[46]</ref> pre-trained ViT-H <ref type="bibr" target="#b31">[32]</ref> image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine. For all other model and training details, such as hyperparameters, refer to §B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Zero-Shot Single Point Valid Mask Evaluation</head><p>Task. We evaluate segmenting an object from a single foreground point. This task is ill-posed as one point can refer to multiple objects. Ground truth masks in most datasets do not enumerate all possible masks, which can make automatic metrics unreliable. Therefore, we supplement the standard mIoU metric (i.e., the mean of all IoUs between predicted and ground truth masks) with a human study in which annotators rate mask quality from 1 (nonsense) to 10 (pixel-perfect). See §E.1, §F, and §H for additional details.</p><p>By default, we sample points from the "center" of ground truth masks (at a maximal value of the mask's interior distance transform), following the standard evaluation protocol in interactive segmentation <ref type="bibr" target="#b89">[90]</ref>. Since SAM is capable of predicting multiple masks, we evaluate only the model's most confident mask by default. The baselines are all single-mask methods. We compare mainly to RITM <ref type="bibr" target="#b89">[90]</ref>, a strong interactive segmenter that performs best on our benchmark compared to other strong baselines <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>ADE20K <ref type="bibr" target="#b113">[115]</ref> BBBC038v1 <ref type="bibr" target="#b10">[11]</ref> Cityscapes <ref type="bibr" target="#b23">[24]</ref> DOORS <ref type="bibr" target="#b77">[78]</ref> DRAM <ref type="bibr" target="#b22">[23]</ref> EgoHOS <ref type="bibr" target="#b109">[111]</ref> GTEA <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref> Hypersim <ref type="bibr" target="#b83">[84]</ref> IBD <ref type="bibr" target="#b15">[16]</ref> iShape <ref type="bibr" target="#b107">[109]</ref> LVIS <ref type="bibr" target="#b42">[43]</ref> NDD20 <ref type="bibr" target="#b96">[98]</ref> NDISPark <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> OVIS <ref type="bibr" target="#b78">[79]</ref> PPDLS <ref type="bibr" target="#b71">[72]</ref> Plittersdorf <ref type="bibr" target="#b44">[45]</ref> STREETS <ref type="bibr" target="#b88">[89]</ref> TimberSeg <ref type="bibr" target="#b36">[37]</ref> TrashCan <ref type="bibr" target="#b49">[50]</ref> VISOR <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref> WoodScape <ref type="bibr" target="#b108">[110]</ref> PIDRay <ref type="bibr" target="#b100">[102]</ref> ZeroWaste-f <ref type="bibr" target="#b5">[6]</ref> (a) Samples from the 23 diverse segmentation datasets used to evaluate SAM's zero-shot transfer capabilities -20 0 +20 +40 IoU delta at 1 center point GTEA <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b60">61]</ref> TrashCan <ref type="bibr" target="#b49">[50]</ref> DRAM <ref type="bibr" target="#b22">[23]</ref> PIDRay <ref type="bibr" target="#b100">[102]</ref> Cityscapes <ref type="bibr" target="#b23">[24]</ref> WoodScape <ref type="bibr" target="#b108">[110]</ref> IBD <ref type="bibr" target="#b15">[16]</ref> EgoHOS <ref type="bibr" target="#b109">[111]</ref> Plittersdorf <ref type="bibr" target="#b44">[45]</ref> VISOR <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref> NDISPark <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> Hypersim <ref type="bibr" target="#b83">[84]</ref> OVIS <ref type="bibr" target="#b78">[79]</ref> ADE20K <ref type="bibr" target="#b113">[115]</ref> iShape <ref type="bibr" target="#b107">[109]</ref> ZeroWaste-f <ref type="bibr" target="#b5">[6]</ref> STREETS <ref type="bibr" target="#b88">[89]</ref> LVIS <ref type="bibr" target="#b42">[43]</ref> NDD20 <ref type="bibr" target="#b96">[98]</ref> TimberSeg <ref type="bibr" target="#b36">[37]</ref> DOORS <ref type="bibr" target="#b77">[78]</ref> BBBC038v1 <ref type="bibr" target="#b10">[11]</ref> PPDLS <ref type="bibr" target="#b71">[72]</ref> -21.4 -15.0 -6.</p><p>5 -5.8 -2.0 -0.6 -0.3 +0.8 +1.5 +1.8 +2.7 +6.1 +7.0 +7.8 +8.8 +9.1 +17.3 +18.5 +21.1 +28.9 +41.1 +44.7 +46.9 (b) SAM vs. RITM [90] on 23 datasets (c) Mask quality ratings by human annotators (d) Center points (default) (e) Random points Datasets. We use a newly compiled suite of 23 datasets with diverse image distributions, see appendix Table <ref type="table">4</ref> for more details. We use all 23 datasets for mIoU evaluation.</p><p>For the human study, we use the subset listed in Fig. <ref type="figure" target="#fig_4">7c</ref> (due to the resource requirements of such studies). This subset includes both datasets for which SAM outperforms and underperforms RITM according to automatic metrics.</p><p>Results. First, we look at automatic evaluation on the full suite of 23 datasets using mIoU. We compare per-dataset results in Fig. <ref type="figure" target="#fig_4">7b</ref> against RITM. SAM yields higher results on 16 of the 23 datasets, by as much as ⇠47 IoU. We also present an "oracle" result, in which the most relevant of SAM's 3 masks is selected by comparing them to the ground truth, rather than selecting the most confident mask. This reveals the impact of ambiguity on automatic evaluation. In particular, with the oracle to perform ambiguity resolution, SAM outperforms RITM on all datasets.</p><p>Results of the human study are presented in Fig. <ref type="figure" target="#fig_4">7c</ref>. Error bars are 95% confidence intervals (all differences are significant; see §F for details). We observe that the annota-tors consistently rate the quality of SAM's masks substantially higher than the strongest baseline, RITM. An ablated, "ambiguity-unaware" version of SAM with a single output mask has consistently lower ratings. SAM's mean ratings fall between 7 and 9, which corresponds to the qualitative rating guideline: "A high score (7-9): The object is identifiable and errors are small and rare (e.g., missing a small, heavily obscured disconnected component, ...)." These results indicate that SAM has learned to segment valid masks from a single point. Note that for datasets like DRAM and IBD, where SAM is worse on automatic metrics, it receives consistently higher ratings in the human study.</p><p>Fig. <ref type="figure" target="#fig_4">7d</ref> shows additional baselines, SimpleClick <ref type="bibr" target="#b64">[65]</ref> and FocalClick <ref type="bibr" target="#b16">[17]</ref>. As the number of points increases from 1 to 9, we observe that the gap between methods decreases. This is expected as the task becomes easier; also, SAM is not optimized for the very high IoU regime. Finally, in Fig. <ref type="figure" target="#fig_4">7e</ref> we replace the default center point sampling with random point sampling. We observe that the gap between SAM and the baselines grows and SAM is able to achieve comparable results under either sampling method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Zero-Shot Text-to-Mask</head><p>Approach. This experiment is a proof-of-concept of SAM's ability to segment objects from free-form text prompts. While we used the exact same SAM in all prior experiments, for this one SAM's training procedure is modified to make it text-aware, but in a way that does not require new text annotations. Specifically, for each manually collected mask with area larger than 100 2 we extract the CLIP image embedding. Then, during training, we prompt SAM with the extracted CLIP image embeddings as its first interaction. The key observation here is that because CLIP's image embeddings are trained to align with its text embeddings, we can train with image embeddings, but use text embeddings for inference. That is, at inference time we run text through CLIP's text encoder and then give the resulting text embedding as a prompt to SAM (see §E.5 for details).</p><p>Results. We show qualitative results in Fig. <ref type="figure" target="#fig_5">8</ref>. SAM can segment objects based on simple text prompts like "a wheel" as well as phrases like "beaver tooth grille". When SAM fails to pick the right object from a text prompt only, an additional point often fixes the prediction, similar to <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Foundation models. Pre-trained models have been adapted to downstream tasks since the early days of machine learning <ref type="bibr" target="#b95">[97]</ref>. This paradigm has become increasingly important in recent years with a growing emphasis on scale, and such models have recently been (re-)branded as "foundation models": i.e. models that are "trained on broad data at scale and are adaptable to a wide range of downstream tasks" <ref type="bibr" target="#b7">[8]</ref>. Our work correlates well with this definition, though we note that a foundation model for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision. We also contrast one aspect of our approach with <ref type="bibr" target="#b7">[8]</ref>, which emphasizes the role of self-supervised learning in foundation models. While our model is initialized with a selfsupervised technique (MAE <ref type="bibr" target="#b45">[46]</ref>), the vast majority of its capabilities come from large-scale supervised training. In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution.</p><p>Compositionality. Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP <ref type="bibr" target="#b79">[80]</ref> is used as a component in larger systems, such as DALL•E <ref type="bibr" target="#b80">[81]</ref>.</p><p>Our goal is to make this kind of composition straightforward with SAM. We aim to achieve this by requiring SAM to predict a valid mask for a wide range of segmentation prompts. The effect is to create a reliable interface between SAM and other components. For example, MCC <ref type="bibr" target="#b102">[104]</ref> can easily use SAM to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image. In another example, SAM can be prompted with gaze points detected by a wearable device, enabling new applications. Thanks to SAM's ability to generalize to new domains like ego-centric images, such systems work without need for additional training.</p><p>Limitations. While SAM performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that "zoom-in", e.g. <ref type="bibr" target="#b16">[17]</ref>. In general, we expect dedicated interactive segmentation methods to outperform SAM when many points are provided, e.g. <ref type="bibr" target="#b64">[65]</ref>. Unlike these methods, SAM is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, SAM can process prompts in real-time, but nevertheless SAM's overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While SAM can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as <ref type="bibr" target="#b6">[7]</ref>, that we expect to outperform SAM in their respective domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>The Segment Anything project is an attempt to lift image segmentation into the era of foundation models.</p><p>Our principal contributions are a new task (promptable segmentation), model (SAM), and dataset (SA-1B) that make this leap possible. Whether SAM achieves the status of a foundation model remains to be seen by how it is used in the community, but regardless we expect the perspective of this work, the release of over 1B masks, and our promptable segmentation model will help pave the path ahead.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure2: Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are ⇠100 masks per image on average).</figDesc><graphic coords="3,60.43,570.51,120.27,80.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Each column shows 3 valid masks generated by SAM from a single ambiguous point prompt (green circle).</figDesc><graphic coords="4,442.25,241.74,51.68,83.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous prompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores.</figDesc><graphic coords="5,35.38,31.80,141.34,141.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Image-size normalized mask center distributions.</figDesc><graphic coords="6,310.04,72.00,233.88,53.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Point to mask evaluation on 23 datasets. (a) Dataset samples. (b) Mean IoU of SAM and the strongest single point segmenter, RITM<ref type="bibr" target="#b89">[90]</ref>. Due to ambiguity, a single mask may not match ground truth; circles show "oracle" results of the most relevant of SAM's 3 predictions. (c) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). Mask center is used as the prompt. (d, e) mIoU with varying number of points. SAM significantly outperforms prior interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Zero-shot text-to-mask. SAM can work with simple and nuanced text prompts. When SAM fails to make a correct prediction, an additional point prompt can help.</figDesc><graphic coords="9,51.48,186.52,115.76,54.85" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On seeing stuff: the perception of materials by humans and machines. Human vision and electronic imaging VI</title>
		<author>
			<persName><surname>Edward H Adelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2010-04-19">2010. 4, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">BERT pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ZeroWaste dataset: Towards deformable object segmentation in cluttered scenes</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Bashkirova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Akl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fadi</forename><surname>Alladkani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Ablavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ilastik: interactive machine learning for (bio)image analysis</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Kutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorben</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">N</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><forename type="middle">X</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schiegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janez</forename><surname>Ales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Rudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kemal</forename><surname>Eren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">I</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buote</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fynn</forename><surname>Beuttenmueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Wolny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kreshuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative interaction training for segmentation editing networks</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Bredell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MICCAI</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">W</forename><surname>Karhohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">A</forename><surname>Cimini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanelle</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cherkeng</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Mcquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="1986">1986. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end object detection with Transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic image colorization via multimodal predictions</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harsh Agrawal, Aroma Mahendru, and Dhruv Batra. Object-proposal evaluation protocol is&apos; gameable</title>
		<author>
			<persName><forename type="first">Neelima</forename><surname>Chavali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">instance segmentation of MVS buildings</title>
		<author>
			<persName><forename type="first">Jiazhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghua</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Nan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 8, 17, 18, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FocalClick: towards practical interactive image segmentation</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manni</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2009">2022. 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perpixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain adaptation for traffic density estimation</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Night and day instance segmented park (NDIS-Park) dataset: a collection of images taken by day and by night for vehicle detection, segmentation and counting in parking areas</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Ciampi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic segmentation in art paintings</title>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yael</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 8, 17, 18, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning parameterized skills</title>
		<author>
			<persName><forename type="first">George</forename><surname>Bruno Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100</title>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 8, 18, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EPIC-KITCHENS VISOR benchmark: Video segmentations and object relations</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Darkhalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amlan</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 8, 17, 18, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Terrance</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<title level="m">Does object recognition work for everyone? CVPR workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crowd-WorkSheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Kivlichan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Amironesei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PhraseClick: toward achieving flexible interactive segmentation by phrase and click</title>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitnick</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2014">2014. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2004">2004. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The validity and practicality of sun-reactive skin types i through vi</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Dermatology</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ning Xu, and Franc ¸ois Pitié</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07932</idno>
	</analytic>
	<monogr>
		<title level="m">Getting to 99% accuracy in interactive segmentation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Instance segmentation for autonomous log grasping in forestry operations</title>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Gamache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Grondin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Franc ¸ois Pomerleau, and Philippe Giguère</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Datasheets for datasets</title>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">Daumé</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Crawford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Kumar Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayant</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Zhongcong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morrie</forename><surname>Doulaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Erapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriano</forename><surname>Fragomeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qichen</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abrham</forename><surname>Gebreselasie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weslie</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jachym</forename><surname>Khoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Landini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghava</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Modhugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tullie</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takumi</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Nishiyasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><forename type="middle">Ruiz</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Merey</forename><surname>Puentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leda</forename><surname>Ramazanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Somasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Southerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">C V</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanbyul</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kris</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><surname>Malik</surname></persName>
		</author>
		<editor>Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu,</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>Ego4D: Around the World in 3,000 Hours of Egocentric Video. CVPR</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2019">2019. 2, 6, 7, 8, 17, 18, 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiple choice learning: Learning to produce multiple structured outputs</title>
		<author>
			<persName><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SOCRATES: Introducing depth in visual wildlife monitoring using stereo vision</title>
		<author>
			<persName><surname>Timm Haucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hjalmar</surname></persName>
		</author>
		<author>
			<persName><surname>Kühl</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Steinhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2022. 5, 7, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">TrashCan: A semantically-segmented dataset towards visual detection of marine debris</title>
		<author>
			<persName><forename type="first">Jungseok</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junaed</forename><surname>Sattar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08097</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Deep networks with stochastic depth. ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Oneformer: One transformer to rule universal image segmentation</title>
		<author>
			<persName><forename type="first">Jitesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mangtik</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.06220</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning open-world object proposals without learning to classify</title>
		<author>
			<persName><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<editor>
			<persName><forename type="first">So</forename><surname>Kweon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV, 2020. 2, 6</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2022">2022. 5, 13, 19, 21</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Coco</forename><surname>Microsoft</surname></persName>
		</author>
		<title level="m">Common objects in context. ECCV</title>
		<imprint>
			<date type="published" when="2014">2014. 2, 4, 6, 7, 16</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Sim-pleClick: Interactive image segmentation with simple vision transformers</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11006</idno>
		<imprint>
			<date type="published" when="2009">2022. 7, 8, 9</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Gelatinous zooplankton biomass in the global oceans: geographic variation and environmental drivers</title>
		<author>
			<persName><forename type="first">Cathy H</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><surname>Hollyhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">M</forename><surname>Robert H Condon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kylie</forename><forename type="middle">A</forename><surname>Kelly L Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Pitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Schildhauer</surname></persName>
		</author>
		<author>
			<persName><surname>Regetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Ecology and Biogeography</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Sabarinath Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep extreme cut: From extreme points to object segmentation</title>
		<author>
			<persName><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Finely-grained annotated datasets for imagebased plant phenotyping</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanno</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotirios</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<title level="m">Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the conference on fairness, accountability, and transparency</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName><surname>Dim P Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">EDTER: Edge detection with transformer</title>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingji</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">DOORS: Dataset fOr bOuldeRs Segmentation</title>
		<author>
			<persName><forename type="first">Mattia</forename><surname>Pugliatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Topputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2022">2022. 8, 18, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2009">2021. 1, 2, 4, 5, 7, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Zero-shot textto-image generation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2009">2021. 1, 4, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atulit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Paczan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A step toward more inclusive people annotations for fairness</title>
		<author>
			<persName><forename type="first">Candice</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utsav</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">LightFace: A hybrid deep face recognition framework</title>
		<author>
			<persName><forename type="first">Sefik</forename><surname>Ilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serengil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Ozpinar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASYU</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">HyperExtended LightFace: A facial attribute analysis framework</title>
		<author>
			<persName><forename type="first">Sefik</forename><surname>Ilkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serengil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Ozpinar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICEET</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">TextonBoost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">STREETS: A novel camera network dataset for traffic flow</title>
		<author>
			<persName><forename type="first">Corey</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Reviving iterative training with mask guidance for interactive segmentation</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><forename type="middle">A</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2022">2022. 5, 7, 8, 14, 15, 17, 22</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Multi-stream deep neural networks for RGB-D egocentric action recognition</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017. 2019</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>Action recognition in RGB-D egocentric videos</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">The world by income and regions</title>
		<ptr target="https://datatopics.worldbank.org/world-development-indicators/the-world-by-income-and-region.html.16" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>The World Bank</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Is learning the n-th thing any easier than learning the first? NeurIPS</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation</title>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Trotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Sharpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsten</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Stephen</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Burville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per</forename><surname>Berggren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13359</idno>
		<imprint>
			<date type="published" when="2020">2020. 8, 17, 18, 22</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<ptr target="https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator" />
		<title level="m">Greenhouse Gas Equivalencies Calculator</title>
		<imprint>
			<publisher>United States Environmental Protection Agency</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Segmentation as selective search for object recognition. ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Towards real-world prohibited item detection: A largescale x-ray benchmark</title>
		<author>
			<persName><forename type="first">Boying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Open-world instance segmentation: Exploiting pseudo ground truth from learned pairwise affinity</title>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Multiview compressive coding for 3D reconstruction</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krista</forename><surname>Ehinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015. 19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Deep interactive object selection</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 2020 conference on fairness, accountability, and transparency</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><forename type="middle">Zi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Yisheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15068</idno>
		<title level="m">A first step towards irregular shape instance segmentation</title>
		<imprint>
			<date type="published" when="2021">2021. 8, 18, 22</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">WoodScape: A multi-task, multicamera fisheye dataset for autonomous driving</title>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciarán</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padraig</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">O</forename><surname>'dea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Uricár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Amende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Finegrained egocentric hand-object segmentation: Dataset, model, and applications</title>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ECCV</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">K-Net: Towards unified image segmentation</title>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09457</idno>
		<title level="m">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2019">2019. 2, 7, 8, 18</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
