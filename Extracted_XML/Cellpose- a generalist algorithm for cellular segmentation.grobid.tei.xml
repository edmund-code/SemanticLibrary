<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cellpose: a generalist algorithm for cellular segmentation</title>
				<funder>
					<orgName type="full">Howard Hughes Medical Institute at the Janelia Research Campus</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Science and Business Media LLC</publisher>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-12-14">14 December 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carsen</forename><surname>Stringer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HHMI Janelia Research Campus</orgName>
								<address>
									<settlement>Ashburn</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HHMI Janelia Research Campus</orgName>
								<address>
									<settlement>Ashburn</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michalis</forename><surname>Michaelos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HHMI Janelia Research Campus</orgName>
								<address>
									<settlement>Ashburn</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marius</forename><surname>Pachitariu</surname></persName>
							<email>pachitarium@janelia.hhmi.org</email>
							<idno type="ORCID">0000-0001-7106-814X</idno>
							<affiliation key="aff0">
								<orgName type="institution">HHMI Janelia Research Campus</orgName>
								<address>
									<settlement>Ashburn</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cellpose: a generalist algorithm for cellular segmentation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Nature Methods</title>
						<title level="j" type="abbrev">Nat Methods</title>
						<idno type="ISSN">1548-7091</idno>
						<idno type="eISSN">1548-7105</idno>
						<imprint>
							<publisher>Springer Science and Business Media LLC</publisher>
							<biblScope unit="volume">18</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="100" to="106"/>
							<date type="published" when="2020-12-14">14 December 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">E8F4A250095C0A83AE1AC1557902B9F6</idno>
					<idno type="DOI">10.1038/s41592-020-01018-x</idno>
					<note type="submission">Received: 3 April 2020; Accepted: 11 November 2020;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2026-01-06T01:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Any methods</term>
					<term>additional references</term>
					<term>Nature Research reporting summaries</term>
					<term>source data</term>
					<term>extended data</term>
					<term>supplementary information</term>
					<term>acknowledgements</term>
					<term>peer review information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>U-Net 'mixes in' the convolutional maps from the downsampling pass. This mixing is typically done by feature concatenation, but we opted for direct summation to reduce the number of parameters. We also replaced the standard building blocks of a U-Net with residual blocks, which have been shown to perform better, and doubled the depth of the network as typically done for residual networks <ref type="bibr" target="#b30">30</ref> . In addition, we used global average pooling on the smallest convolutional maps to obtain a representation of the 'style' of the image (for similar definitions of style, see refs. <ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32</ref> ). We anticipated that images with different styles might need to be processed differently, and therefore fed the style vectors into all the upsampling stages on an image by image basis. These modifications to the U-Net architecture improved performance significantly (Extended Data Fig. <ref type="figure" target="#fig_1">2a-d</ref>).</p><p>We use several test time enhancements to further increase the predictive power of the model: test time resizing (Extended Data Fig. <ref type="figure" target="#fig_2">3</ref>), ROI quality estimation, model ensembling and image tiling (Methods). The run time performance of the algorithm is shown in Supplementary Table <ref type="table">1</ref>.</p><p>Training dataset. We collected images from a variety of sources, primarily via internet searches for keywords such as 'cytoplasm' , 'cellular microscopy' , 'fluorescent cells' and so on. Some of the websites with hits shared the images as educational material, promotional material or news stories, while others contained datasets used in previous scientific publications (for example on OMERO <ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34</ref> ). The dataset consisted primarily of fluorescently labeled proteins that localized to the cytoplasm with or without 4,6-diamidino-2-phenylindole-(DAPI-) stained nuclei in a separate channel (n = 316 images). We also included images of cells from brightfield microscopy (n = 50) and images of membrane-labeled cells (n = 58). Finally, we included a small set of images from other types of microscopy (n = 86), and a small set of nonmicroscopy images that contained large numbers of repeated objects such as fruits, rocks and jellyfish (n = 98). We hypothesized A simulated diffusion process starting from the center of the mask is used to derive spatial gradients that point toward the center of the cell, potentially indirectly around corners. The x and y gradients are combined in a single direction from 0° to 360°. b, Example spatial gradients for cells from the training dataset. c, A neural network is trained to predict the horizontal and vertical gradients, as well as whether a pixel belongs to any cell. The three predicted maps are combined into a gradient vector field. d, The details of the neural network that contains a standard backbone U-net <ref type="bibr" target="#b2">3</ref> to downsample and then upsample the feature maps, with skip connections between layers of the same size and global skip connections from the image styles, computed at the lowest resolution, to all successive computations. e, At test time, the predicted gradient vector fields are used to construct a dynamical system with fixed points whose basins of attraction represent the predicted masks. Informally, every pixel 'tracks the gradients' toward their eventual fixed point. f, All the pixels that converge to the same fixed point are assigned to the same mask.</p><p>that the inclusion of such images in the training set would allow the network to generalize more widely and more robustly.</p><p>We visualized the structure of this dataset by applying t-distributed stochastic neighbor embedding (t-SNE) to the image styles learned by the neural network (Fig. <ref type="figure" target="#fig_1">2</ref>) <ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36</ref> . Images were generally placed in the embedding according to the categories defined above, with the noncell image categories scattered across the entire embedding space. To manually segment these images, we developed a graphical user interface in Python (Extended Data Fig. <ref type="figure" target="#fig_0">1</ref>) that relies on PyQt and pyqtgraph <ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27</ref> . The interface enables quick switching between views such as outlines, filled masks and image color channels, as well as easy image navigation such as zooming and panning. A new mask is initiated with a right-click, and gets completed automatically when the user returns within a few pixels of the starting area. This interface allowed human operators to segment 300-600 objects per hour, and is part of the code we are releasing with the Cellpose package. The segmentation algorithm can also be run in the same interface, allowing users to easily curate and contribute their own data to our training dataset.</p><p>Within the varied dataset of 608 images, a subset of 100 images were presegmented as part of the Cell Image Library <ref type="bibr" target="#b37">37</ref> . These two-channel images (cytoplasm and nucleus) were from a single experimental preparation and contained cells with complex shapes. We reasoned that the segmentation methods we tested would not overfit on such a large and visually uniform dataset, and may instead fail due to a lack of expressive power such as an inability to precisely follow cell contours or to incorporate information from the nuclear channel. We therefore chose to also train the models on this dataset alone as a 'specialist' benchmark of expressive power.</p><p>Benchmarks. Trained on our entire library of images, the 'generalist' Cellpose model achieved good performance on the test set (Fig. <ref type="figure" target="#fig_2">3</ref>). We also trained Cellpose as a specialist model on the Cell Image Library alone ('specialized data' , Fig. <ref type="figure" target="#fig_3">4a</ref>,<ref type="figure">b</ref>), and compared this to the generalist model (Fig. <ref type="figure" target="#fig_3">4ab</ref>). We included in the comparisons several other state-of-the-art methods: Stardist 18 , Mask R-CNN <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> and two-and three-class U-Net models <ref type="bibr" target="#b2">3</ref> . Example segmentations for Stardist and Mask R-CNN are shown in Extended Data Figs. <ref type="figure" target="#fig_3">4</ref> and <ref type="figure" target="#fig_4">5</ref>. On test images, we matched the predictions of the algorithms to the true masks at different thresholds of matching precision based on the standard intersection over union metric (IoU). We evaluated performance with the average precision metric (AP), computed from the number of true positives, TP, false positives, FP and false negatives, FN, as AP ¼ TP TPþFPþFN I . Similar results were obtained using boundary prediction metrics (Extended Data Fig. <ref type="figure">6a</ref>,<ref type="figure">b</ref>) and the aggregate Jaccard index (Extended Data Fig. <ref type="figure">6c</ref>,<ref type="figure">d</ref>).</p><p>Cellpose substantially outperformed previous methods at all matching thresholds, when trained and tested on the specialized data. For example, at the commonly used IoU threshold of 0.5, Cellpose correctly matched 485 out of 521 total ground-truth ROI and gave only 18 false positives. This corresponded to an average precision of 0.91, compared to 0.76 for Stardist and 0.80 for Mask R-CNN. At higher IoUs, which benchmark the ability to precisely follow cell contours, the relative improvement of Cellpose compared to other methods grew even larger. This analysis thus shows that Cellpose has enough expressive power to capture complicated shapes (Fig. <ref type="figure" target="#fig_3">4d</ref>).</p><p>However, all models trained on the specialized data alone performed poorly on the full dataset (Fig. <ref type="figure" target="#fig_3">4e</ref>), motivating the need for a generalist algorithm. On the specialized data alone (the Cell Image Library), the generalist Cellpose model matched the performance of the specialist model (Fig. <ref type="figure" target="#fig_3">4f</ref>). On the generalized data alone, the generalist Cellpose model had an average precision of 0.77 at a threshold of 0.5, significantly outperforming Mask R-CNN and Stardist that had average precisions of 0.61 and 0.61 respectively (Fig. <ref type="figure" target="#fig_3">4g</ref>). We also tested if the inclusion of specialized images in the training of the generalist model impaired its performance on generalized images, and found that they in fact helped considerably (Extended Data Fig. <ref type="figure" target="#fig_1">2e</ref>). Altogether, these results show that the inclusion of more images in the training set does not saturate the capacity of the neural network, indicating that Cellpose might have even more spare capacity for additional training data, which we hope to identify via community contributions.</p><p>We next wanted to determine what types of image and cell are more challenging to segment. For this, we examined the predictions of the models on each of the image categories in our generalized test set (Fig. <ref type="figure" target="#fig_4">5a</ref>). Across image categories, Cellpose had better performance on nonfluorescent images of cells, on cell membranes and on the Cell Image Library (Fig. <ref type="figure" target="#fig_2">3f</ref>), likely reflecting the higher homogeneity of these image sets. This led us to hypothesize images that contain more variability across cells, for example in cell sizes, would be more difficult to segment. However, we found no relation between segmentation performance and the homogeneity of cell sizes (Fig. <ref type="figure" target="#fig_4">5b</ref>). Finally, we asked whether the convexity of a cell influences segmentation performance, by grouping cells into low, medium and high convexity (Fig. <ref type="figure" target="#fig_4">5b</ref>). We found that all models performed better on cells with high convexity compared to low convexity (Fig. <ref type="figure" target="#fig_4">5d</ref>), and the performance penalty for low versus high convexity was similar across models.</p><p>Finally, we assembled a large dataset of images of nuclei, by combining presegmented datasets from several previous studies, including the Data Science Bowl kaggle competition <ref type="bibr" target="#b14">15</ref> . Because nuclear shapes are simpler and more uniform, this dataset did not have as much variability as the dataset of cells (see the t-SNE embedding of the segmentation styles in Extended Data Fig. <ref type="figure">7a</ref>). Cellpose outperformed the other methods on this dataset by smaller amounts, likely reflecting the reduced difficulty of this segmentation problem (Extended Data Figs. <ref type="figure">7b</ref>,<ref type="figure">c</ref> and <ref type="figure">8</ref>). 3D segmentation. Our last contribution was to generalize Cellpose for 3D segmentation. This task usually requires 3D training data, which is much more difficult to obtain than 2D training data as it requires one 2D segmentation for every slice in the volume. Thus, a cell that spanned 30 pixels, the median diameter in our dataset, would take 30 times longer to manually segment in 3D. An alternative approach would be to extend pretrained, generalist 2D models to 3D by running the neural networks on every 2D slice. The 3D segmentation step that would follow is straightforward for some models such as the U-Nets, where the postprocessing step of finding connected components can be performed similarly in 3D and 2D.</p><p>However, this type of extension is not available for models with more complex postprocessing steps, such as Cellpose, Stardist and Mask R-CNN. We therefore designed a new method for extending Cellpose to 3D, using only the trained 2D model and no additional 3D training data. For a test volume, we ran Cellpose on all xy, xz and yz slices independently (Fig. <ref type="figure">6a</ref>,<ref type="figure">b</ref>). For each point, this procedure generated two estimates of the gradient in x, y and z (that is, six total predictions), which we then averaged together to obtain a complete set of 3D vector gradients. To generate ROI, we then ran the gradient vector tracking step in 3D, followed by a clustering of pixels that converge to the same fixed points (Fig. <ref type="figure">6e</ref>).</p><p>We compared Cellpose3D to the 3D extensions of the 2D U-Net models (Fig. <ref type="figure">6e</ref>). We also compared against a different 3D approach of 'stitching' together sequential 2D segmentations from each xy plane. This approach can be applied to any 2D method, including the generalist Stardist and Mask R-CNN models. Finally, we used ilastik to generate a 3D segmentation pipeline specific for this volume. The parameters of ilastik were chosen manually to give good performance on a subset of the volume that was not used for testing. Like Cellpose3D, ilastik did not require a large 3D training dataset, and thus can be trained and deployed easily by nonexperts.</p><p>We evaluated the performance of Cellpose3D and the other models on a manually annotated 3D test volume in which the DNA and RNA were costained to serve as nuclear and cytoplasmic markers, respectively (Fig. <ref type="figure">6f</ref>). The human annotator found 183 cells, while Cellpose3D predicted 172 cells, with 17 false positives at an IoU threshold of 0.5 (Fig. <ref type="figure">6g</ref>, Videos 1 and 2). At all IoU thresholds, Cellpose3D outperformed the ilastik pipeline. Cellpose3D also outperformed the 2D-stitched versions of Cellpose and other models by a substantial margin (Fig. <ref type="figure">6g</ref>). Finally, Cellpose3D outperformed the U-Net extensions to 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>discussion</head><p>Here we introduced Cellpose, a generalist model that can segment many types of cell without requiring parameter adjustments, new training data or further model retraining. Cellpose uses two main innovations: a reversible transformation from training set masks to vector gradients that can be predicted by a neural network and a large segmented dataset of varied images of cells. In addition, multiple smaller improvements to the basic approach led to a significant cumulative performance increase: we developed methods to use an image 'style' for changing the neural network computation on an image by image basis, to validate segmented ROI, to average the predictions of multiple models, to resize images to a common object size and to average model predictions in overlapping tiles. Finally, we introduced a new method for 3D cell segmentation that reuses the 2D model and does not require new 3D-labeled data.</p><p>Our approach to the segmentation problem allows anyone to contribute new training data to improve Cellpose for themselves and for others. We encourage users to contribute up to a few manually segmented images of the same type, which we will use to periodically retrain a single generalist Cellpose model for the community. Cellpose has high expressive power and high capacity, as shown by its ability to segment cells with complex protuberances such as elongated dendrites, and even noncell objects such as rocks and jellyfish. We therefore expect Cellpose to successfully incorporate new training data that has a passing similarity to an image of cells and consists of collections of similar-looking objects. Our initial tests indicate that the inclusion in the training set of unusual, nonbiological images especially helps for generalization to out-of-sample images. However, generalizing to out-of-sample data is a notoriously difficult problem for neural networks (for example, Fig. <ref type="figure" target="#fig_3">4e</ref>), and more work will be needed to best take advantage of new types of training data. The IoU threshold quantifies the match between a predicted mask and a ground-truth mask, with 1 indicating a pixel-perfect match and 0.5 indicating as many correctly matched pixels as there were missed and false positive pixels. The average precision score is computed from the proportion of matched and missed masks. To gain an intuition for the range of these scores, refer to the reported values in b,c as well as Fig. <ref type="figure" target="#fig_2">3</ref>. e, Performance of the specialist models on generalized data (n = 57 test images). f,g, Same as d,e for generalist models.</p><p>While our goal here was to develop generalist algorithms for 2D and 3D segmentation, it is also possible to train Cellpose on specialized types of data, such as cryo-EM images <ref type="bibr" target="#b38">38</ref> , provided that a large training dataset exists. Similarly, a 3D version of Cellpose could be trained directly on a 3D ground-truth dataset, which may result in better performance <ref type="bibr" target="#b39">39</ref> . Other extensions of Cellpose are possible, for example to cell tracking <ref type="bibr" target="#b40">40</ref> , which could be addressed by adding a 'temporal gradient' dimension to the spatial gradient dimensions. Finally, further algorithmic improvements could be targeted toward low convexity cells, which were the least well segmented by all algorithms (Fig. <ref type="figure" target="#fig_4">5d</ref>). Combined with new histology methods such as spatial transcriptomics <ref type="bibr" target="#b41">41</ref> , Cellpose has the potential to aid and enable new approaches in quantitative single-cell biology that are reproducible and scalable to large datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The Cellpose code library is implemented in Python 3 (ref. <ref type="bibr" target="#b20">20</ref> ), using numpy, scipy, numba, opencv and the deep learning package mxnet <ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref> . The graphical user interface additionally uses PyQt and pyqtgraph <ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27</ref> . The figures were made using matplotlib and jupyter-notebook <ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45</ref> .</p><p>Datasets. Animals. For data collected by us, all experimental procedures were approved by the Janelia IACUC (Institutional Animal Care and Use Committee) at Janelia Research Campus, Howard Hughes Medical Institute. All mice were housed at 21 °C ± 2 °C and humidity 30-70% in a 12:12 light-dark cycle.</p><p>Specialized data. This dataset consisted of 100 fluorescent images of cultured neurons with cytoplasmic and nuclear stains obtained from the Cell Image Library <ref type="bibr" target="#b37">37</ref> . Each image has a manual cytoplasmic segmentation. We corrected some of the manual segmentation errors where touching neurons were joined, and removed any overlaps in the segmentation so that each pixel in the image was assigned to a maximum of one cytoplasmic mask. A small number of dendrites were cut off by this process, but the bulk of their cell bodies was unmodified.</p><p>Generalized data. The full dataset included the 100 images described above, as well as 508 additional images from various sources detailed below. All these extra images were fully manually segmented by a single human operator (M.M.). Of these images, 69 were reserved for testing. Two hundred and sixteen of the images contained cells with fluorescent cytoplasmic markers. We used BBBC020 from the Broad Bioimage Benchmark Collection <ref type="bibr" target="#b46">46</ref> , which consisted of 25 images of bone-marrow derived macrophages from C57BL/6 mice. We also used BBBC007v1 image set v.1 (ref. <ref type="bibr" target="#b47">47</ref> ), which consisted of 15 fluorescent images of Drosophila melanogaster Kc167 cells with stains for DNA and actin. We imaged in vivo mouse cortical and hippocampal cells expressing GCaMP6 using a two-photon microscope, and included eight images (mean activity) of such data (two C57BL/6 female mice 3-8 months old). We also used ten images from confocal imaging of mouse cortical neurons with cytoplasmic and nuclear markers from a similar dataset to that which we used for 3D segmentation. The other images in this set were obtained through Google image searches.</p><p>Fifty of the images were taken with standard brightfield microscopy. There were four images shared via OMERO <ref type="bibr" target="#b33">33</ref> of pancreatic stem cells on a polystyrene substrate taken using a light microscope <ref type="bibr" target="#b48">48</ref> . The other images in this set were obtained through Google image searches.</p><p>In 58 of the images, the cell membrane was fluorescently labeled. We used the Micro-Net image set, which consisted of 40 fluorescent images of mouse pancreatic exocrine cells and endocrine cells with a membrane marker (Ecad-FITC) and a nuclear marker (DAPI) <ref type="bibr" target="#b49">49</ref> . Because the nuclear marker did not appear in focus with the membrane labels, we discarded this channel and resegmented all images according to the membrane marker exclusively. The other images in this set were obtained through Google image searches.</p><p>Eighty-six of the images were other types of microscopy sample that were either not cells or cells with atypical appearances. These images were obtained through Google image search.</p><p>Ninety-eight of the images were nonmicroscopy images of repeating objects. These images were obtained through Google image search, and include images of fruits, vegetables, artificial materials, fish and reptile scales, starfish, jellyfish, sea urchins, rocks, sea shells and so on.</p><p>Nucleus data. This dataset consisted of 1,139 images with manual segmentations from various sources, 111 of which were reserved for testing. We did not segment any of these images ourselves.</p><p>We used image set BBBC038v1 (ref. <ref type="bibr" target="#b14">15</ref> ), available from the Broad Bioimage Benchmark Collection. For this image set, we used the unofficial corrected manual labels provided by Konstantin Lopuhin <ref type="bibr" target="#b50">50</ref> .</p><p>We used image set BBBC039v1 (ref. <ref type="bibr" target="#b14">15</ref> ), which consists of 200 fluorescent images of U2OS osteosarcoma cells with the Hoechst stain. Some of these images overlap with the BBBC038v1, so we only used the 157 unique images.</p><p>We used image set MoNuSeg, which consists of 30 H&amp;E stained histological images of various human organs <ref type="bibr" target="#b51">51</ref> . Because the images contain many small nuclei (roughly 700 per image), we divided each of these images into nine separate images. We inverted the polarity of these images so that foreground nuclear pixels had higher intensity values than the background, which is more similar to fluorescence images.</p><p>We also used the image set from ISBI 2009, which consists of 97 fluorescent images of U2OS cells and NIH3T3 cells <ref type="bibr" target="#b52">52</ref> .</p><p>3D data. The tissue was acquired from the cortex of a 10-week-old C57BL/6 male mouse. We used a DNA stain (DAPI) and an RNA stain (fluorescent oligonucleotide probe against the 28S ribosomal RNA) to label the nucleus and cytoplasm, respectively, for a volume of cortical neurons. The tissue was first expanded by a factor of two and digested to remove all proteins <ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54</ref> . The tissue was imaged on a Zeiss Z1 lightsheet microscope using the ZEN Microscope Software. A small cube (roughly 190 × 190 × 190 μm 3 ) was manually labeled by a human annotator in the Cellpose graphical user interface. We allowed the annotator to potentially skip 'easy' planes while drawing a cell, with an algorithm automatically interpolating the shapes of cells in those planes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary vector flow representation.</head><p>Central to the Cellpose model is an auxiliary representation that converts the cell masks to two images or 'maps' of the same size as the original image. These maps are similar to the vector fields predicted by tracking models such as OpenPose <ref type="bibr" target="#b55">55</ref> , which point from object parts to other object parts and are used in object tracking and coarse segmentation. In our case, the vector fields lead all pixels within a cell toward its center. The gradients do not necessarily point directly toward the center of the cell <ref type="bibr" target="#b56">56</ref> , because that direction might intersect cell boundaries. Instead, the gradients are designed so that locally they translate pixels to other pixels inside cells, and globally over many iterations translate pixels to the eventual fixed points of the gradient vector field. These fixed points are in turn chosen by us to be the cell centers. Since all pixels from the same cells end up at the same cell center, it is easy to recognize which pixels should be grouped together into ROI, by assigning the same cell label to pixels with the same fates. The task of the neural network is to predict these gradients from the raw image, which is potentially a highly nonlinear transformation. Note the similarity to gradient flow tracking methods, where the gradients are computed directly as derivatives of the raw image brightness <ref type="bibr" target="#b29">29</ref> .</p><p>To create a vector flow field with the properties described above, we turned to a heat diffusion simulation. We define the 'center' of each cell as the pixel in a cell that was closest to the median values of horizontal and vertical positions for pixels in that cell. In the heat diffusion simulation, we introduce a heat source at the center pixel, which adds a constant value of one to that pixel's value at each iteration. Every pixel inside the cell gets assigned the average value of pixels in a 3 × 3 square surrounding it, including itself, at every iteration, with pixels outside of a mask being assigned to zero at every iteration. In other words, the boundaries of the cell mask are 'leaky' . This process gets repeated for N iterations, where N is chosen for each mask as twice the sum of its horizontal and vertical range, to ensure that the heat dissipates to the furthest corners of the cell. The distribution of heat at the end of the simulation approaches the equilibrium distribution. We use this final distribution as an energy function, whose horizontal and vertical gradients represent the two vector fields of our auxiliary vector flow representation. We obtained similar, but very slightly worse, performance by using other definitions of 'center' such as the 2D medoid, and other ways to generate an energy function such as the solution of the Poisson equation <ref type="bibr" target="#b57">57</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep neural network.</head><p>The input to the neural network was a two-channel image with the primary channel corresponding to the cytoplasmic label, and the optional secondary channel corresponding to nuclei, which in all cases was a DAPI stain. When a second channel was not available, it was replaced with an image of zeros. Raw pixel intensities were linearly scaled for each image so that the 1 and 99 percentiles corresponded to 0 and 1. Downsampling pass. The neural network was composed of a downsampling pass followed by an upsampling pass, as typical in U-Nets <ref type="bibr" target="#b2">3</ref> . Both passes were composed of four spatial scales, with each scale composed of two residual blocks and each residual block composed of two convolutions with filter size 3 × 3, as is typical in residual networks <ref type="bibr" target="#b30">30</ref> . This resulted in four convolutional maps per spatial scale, followed by maximum pooling to downsample the feature maps. Each convolutional map was preceded by a batchnorm + relu operation, in the order recommended by He et al. <ref type="bibr" target="#b58">58</ref> . The skip connections were additive identity mappings for the second residual block at each spatial scale. For the first residual block, we used 1 × 1 convolutions for the skip connections, as in the original residual networks <ref type="bibr" target="#b30">30</ref> , because these convolutions follow a downsampling or upsampling operation where the number of feature maps changes. In mathematical notation, the following operations were performed in each layer on the downsampling pass:</p><formula xml:id="formula_0">x 0 t ¼ D 2 ´2ðx t�1 Þ x  t ¼ F ðF ðx 0 t ÞÞ þ P 1 ´1ðx 0 t Þ x t ¼ F ðF ðx  t ÞÞ þ x  t ;</formula><p>where D 2x2 indicates the downsampling operation, F I combines the batchnorm, relu and convolution steps, and P 1 ´1 I is the 1 × 1 convolution. Different applications of F I are understood to have different parameters, which were fit by gradient descent.</p><p>Image style. In between the downsampling and upsampling steps we computed an image style <ref type="bibr" target="#b31">31</ref> , defined as the global average pool of each feature map <ref type="bibr" target="#b32">32</ref> , resulting in a 256-dimensional feature vector for each image. To account for differences in cell density across images, we normalized the feature vector to a norm of one for every image. This style vector was passed as input to the residual blocks on the upsampling pass, after projection to a suitable number of features equal to the number of convolutional feature maps of the corresponding residual block, as described below. In mathematical notation, the style vector s * was computed as follows from x 4 :</p><formula xml:id="formula_1">s½k ¼ P i;j x 4 ½k; i; j s  ¼ s=jjsjj</formula><p>where k indexes the feature maps, and i, j are indices for the horizontal and vertical dimensions of the convolutions.</p><p>Upsampling pass. On the upsampling pass, we followed the typical U-Net architecture, where the convolutional layers after an upsampling operation take as input not only the previous feature maps, but also the feature maps from the equivalent level in the downsampling pass <ref type="bibr" target="#b2">3</ref> . We depart from the standard feature concatenation in U-Nets and combine these feature maps additively on the second out of four convolutions per spatial scale <ref type="bibr" target="#b59">59</ref> . The last three convolutions, but not the first one, also had the style vectors added after a suitable linear projection to match the number of feature maps and a global broadcast to each position in the convolution maps. In mathematical notation, each layer in the upsampling pass performed the following operations:</p><formula xml:id="formula_2">z 0 t ¼ U 2 ´2ðz tþ1 Þ z  t ¼ Gðs  ; F ðz 0 t Þ þ x t Þ þ P 1 ´1ðz 0 t Þ z t ¼ Gðs  ; Gðs  ; z  t ÞÞ þ z  t ;</formula><p>where U 2 ´2 I is the upsampling operation and was skipped for computing z 4 from x 4 , while F I is the same operation from the downsampling step and G additionally adds a dense, broadcasted projection from the style vector:</p><formula xml:id="formula_3">GðxÞ½k; i; j ¼ F ðxÞ½k; i; j þ Pðs  Þ½k:</formula><p>Finally, the last convolutional map on the upsampling pass was given as given as input to a 1 × 1 layer of three output convolutional maps. The first two of these were used to directly predict the horizontal and vertical gradients using an L2 loss. The third output map was passed through a sigmoid and used to predict the probability that a pixel is inside or outside of a cell with a cross-entropy loss. To match the relative contributions of the L2 loss and cross-entropy loss, we multiplied the gradients by a factor of five. In mathematical notation, the final outputs y are computed as:</p><formula xml:id="formula_4">y ¼ F ðz 1 Þ</formula><p>and the cost function for Cellpose was:</p><formula xml:id="formula_5">Cost ¼ jjy 0 � 5Hjj 2 þ jjy 1 � 5Vjj 2 þ Lðσðy 2 Þ; PÞ</formula><p>where y k subselected the feature map k, H and V were the ground-truth horizontal and vertical flow fields, P was the inside/outside binary map, L was the binary cross-entropy loss and σðxÞ ¼ 1=ð1 þ expð�xÞÞ I was the sigmoid function. We built and trained the deep neural network using mxnet <ref type="bibr" target="#b25">25</ref> .</p><p>Benchmarks for submodules. We verified that it was beneficial to make these architectural changes over the more standard U-Net, by comparing the performance of different models with various elements disabled. To obtain a standard U-Net from our architecture, we needed to apply three changes: (1) disabling the style; (2) replacing residual blocks with standard convolutions and (3) replacing feature addition with concatenation on the downsampling pass. We first checked the performance penalty from each change. Both the style and residual blocks were beneficial to Cellpose, because disabling them reduced the performance of the model substantially (Extended Data Fig. <ref type="figure" target="#fig_1">2a</ref>,<ref type="figure">b</ref>). The feature addition step was able to replace concatenation without loss of performance, even though it reduced the number of parameters substantially (Extended Data Fig. <ref type="figure" target="#fig_1">2a</ref>,<ref type="figure">b</ref>). Finally, all three changes combined led to the standard U-Net architecture, which was also significantly worse than the Cellpose architecture (Extended Data Fig. <ref type="figure" target="#fig_1">2a</ref>,<ref type="figure">b</ref>). The performance differences from disabling each feature were approximately additive (Extended Data Fig. <ref type="figure" target="#fig_1">2a-d</ref>).</p><p>Training. The networks were trained for 500 epochs with stochastic gradient descent with a learning rate of 0.2, a momentum of 0.9, batch size of eight images and a weight decay of 0.00001. The learning rate was started at zero and annealed linearly to 0.2 over the first ten epochs to prevent initial instabilities. The value of the learning rate was chosen to minimize training set loss (we also tested 0.01, 0.05, 0.1 and 0.4). At epoch 400, the learning rate annealing schedule was started, reducing the learning rate by a factor of two every ten epochs. For all analyses in the paper we used a base of 32 feature maps in the first layer, growing by a factor of two at each downsampling step and up to 256 feature maps in the layer with the lowest resolution. Separate experiments on a validation set held out from the main training dataset confirmed that increases to a base of 48 feature maps were not helpful, while decreases to 24 and 16 feature maps hurt the performance of the algorithm.</p><p>Image augmentations for training. On every epoch, the training set images are randomly transformed together with their associated vector fields and pixel inside/ outside maps. For all algorithms, we used random rotations, random scalings and random translations, and then sampled a 224 × 224 image from the center of the resultant image. For Cellpose, the scaling was composed of a scaling to a common size of cells, followed by a random scaling factor between 0.75 and 1.25. For Mask R-CNN and Stardist, the common size resizing was turned off because these methods cannot resize their predictions. Correspondingly, we used a larger range of scale augmentations between 0.5 and 1.5. Note that Mask R-CNN additonally uses a multi-scale training model based on the size of the objects in the image.</p><p>The random rotations were uniformly drawn from 0° to 360°. Random translations were limited along x and y to a maximum amplitude of (l x -224)/2 and (l y -224)/2, where l y and l x are the sizes of the original image. This ensures that the sample is taken from inside the image. Rotations, but not translations and scalings, result in changes to the direction of vectors. We therefore rotated the mask flow vectors by the same angles the images were rotated.</p><p>Mask recovery via gradient flow tracking. The output of the neural network after tiling is a set of three maps: the horizontal gradients, the vertical gradients and the pixel probabilities. The next step is to recover the masks from these. First, we threshold the pixel probability map and only consider pixels above a threshold of 0.5. For each of these pixels, we run a dynamical system starting at that pixel location and following the spatial derivatives specified by the horizontal and vertical gradient maps. We use finite differences with a step size of one. Note that we do not renormalize the predicted gradients, but the gradients in the training set have unit norm, so we expect the predicted gradients to be on the same scale. We run 200 iterations for each pixel, and at every iteration we take a step in the direction of the gradient at the nearest grid location. Following convergence, pixels can be easily clustered according to the pixel they end up at. For robustness, we also extend the clusters along regions of high-density of pixel convergence. For example, if a high-density peak occurs at position (x, y), we iteratively agglomerate neighboring positions that have at least three converged pixels until all the positions surrounding the agglomerated region have less than 3 pixels. This ensures success in some cases where the deep neural network is not sure about the exact center of a mask, and creates a region of very low gradient where it thinks the center should be.</p><p>Test time enhancements. We use several test time enhancements to further increase the predictive power of the model: test time resizing, ROI quality estimation, model ensembling and image tiling with overlaps. We describe them briefly in this paragraph and in more detail below. We estimate the quality of each predicted ROI according to the discrepancy between the predicted flows inside that ROI and the optimal, recomputed flows for that mask. We discard ROI for which this discrepancy is large. Model ensembling is performed by averaging the flow predictions of four models with the same architecture trained separately. Image tiling is performed by dividing the image into overlapping tiles of the size of the training set patches (224 × 224 pixels). We use 50% overlaps for both horizontal and vertical tiling, resulting in every pixel being processed four times. We then combine the results by averaging the four flow predictions at every pixel, multiplied with appropriate taper masks to minimize edge effects.</p><p>Test time resizing. While it is easy to resize training set images to a common object size, such resizing cannot be directly performed on test images because we do not know the average size of objects in that image. In practice, a user might be able to quickly specify this value for their own dataset, but for benchmarks we needed an automated object size prediction algorithm. This information is not readily computable from raw images, but we hypothesized that the image style vectors might be a good representation from which to predict the object size. We predicted object sizes in two steps: (1) we train a linear regression model from the style vectors of the training set images, which is a good but not perfect prediction on the test set, and (2) we refine the size prediction on the test set by running Cellpose segmentation after resizing to the object size predicted by the style vectors. Since this segmentation is already relatively good, we can use its mean object size as a better predictor of the true object size. We found that this refined object size prediction reached a correlation of 0.93 and 0.97 with the ground truth on test images, for the cytoplasm and nuclear dataset respectively (Extended Data Fig. <ref type="figure" target="#fig_2">3</ref>). We include this algorithm as a calibration procedure that the user can choose to do either on every image, or just once for their entire dataset. We use this object size to resize test images, run the algorithm to produce the three output maps, and then resize these maps back to the original image sizes before running the pixel dynamics and mask segmentation. For all resizing operations we used standard bilinear interpolation from the OpenCV package <ref type="bibr" target="#b24">24</ref> .</p><p>Image tiling. Image tiling is performed for two reasons: (1) to run Cellpose on images of arbitrary sizes and (2) to run Cellpose on the same image patch size used during training (224 × 224). We start with an image of size l y by l x and divide into a set of overlapping tiles, which covers the entire image. For the analyses in this paper, we use an overlap of 50% along each dimension, but in the code package we allow the overlap to be specified by the user. We then run Cellpose on all tiles, producing three output maps for each. The output maps are then used to reconstitute the gradient and probability maps for the entire image, tapering each tile around its four edges with a sigmoid taper with bandwidth parameter of 7.5 pixels. Note that with 50% overlap, almost all pixels in the final reassembled image are averages of four Cellpose runs for different tiles.</p><p>Mask quality threshold. Note there is nothing keeping the neural network from predicting horizontal and vertical flows that do not correspond to any real shapes at all. In practice, most predicted flows are consistent with real shapes, because the network was only trained on image flows that are consistent with real shapes, but sometimes when the network is uncertain it may output inconsistent flows. To check that the recovered shapes after the flow dynamics step are consistent with real masks, we recompute the flow gradients for these putative predicted masks and compare them to the ones predicted by the network. When there is a large discrepancy between the two sets of flows, as measured by their mean squared difference, we exclude that mask since it is inconsistent. We cross-validate the threshold for this operation on a validation set held out from the main training dataset, and found that a value of 0.4 was optimal.</p><p>3D models. We extend the 2D model to 3D without the need for 3D-labeled data by running the network on various slices of the 3D stack. Slices in xy provide x and y flow information, slices in xz provide x and z flow information and slices in yz provide y and z flow information (Fig. <ref type="figure">6a</ref>). We average over the two estimates of each flow at each pixel (Fig. <ref type="figure">6d</ref>). From each slice, we also predict the cell probability and we average across these three estimates for each pixel. We threshold the cell probability at 0.5 and multiply it by the flows. We then use these flows to run the dynamics to create the mask estimates (Fig. <ref type="figure">6e</ref>). Objects smaller than 10% of the median cell volume (2,000 voxels) were discarded (Fig. <ref type="figure">6f</ref>,<ref type="figure">g</ref>). The default median diameter was used (30 pixels).</p><p>Stitching 2D segmentations into 3D objects. Methods such as Stardist and Mask R-CNN do not have direct extensions to 3D. It was therefore necessary to extend these methods by starting from segmentations of 2D slices throughout the volume. ROI from consecutives slices were marked as 'connected' if they had an IoU ≥ 0.25, a parameter value chosen to maximize the final 3D performance. Using these connections, we then extracted connected components across the z dimension as the 3D ROI. As for the other 3D methods, ROI were dropped if they had a volume of less than 10% of the average ground-truth ROI volume.</p><p>Using ilastik for 3D segmentation. As a comparison to Cellpose for 3D segmentation, we used ilastik 8 in a two-step process. In the first step, we classify each voxel as 'background' , 'nucleus' , or 'cytoplasm' using a supervised algorithm. After manually annotating a small number of voxels, we verified that the algorithm could classify individual pixels reliably on test data. Additional training data did not help at this step, which is typical for the random forest classifiers used by ilastik <ref type="bibr" target="#b7">8</ref> . The following features were used for the classification: Gaussian smoothing (sigma = 0, 3, 1, 3.5 and 10), Gaussian gradient magnitude (sigma = 0.7, 1.6 and 5), difference of Gaussians (sigma = 0.7, 1.6 and 5), structure tensor eigenvalues (sigma = 0.7, 1.6 and 5) and Hessian of Gaussian eigenvalues (sigma = 0.7, 1.6 and 5).</p><p>Following the per-pixel classification, individual objects (that is, cells) were identified using the hysteresis thresholding method. This step is unsupervised and thus does not require training data. The 'nucleus' and 'cytoplasm' probability maps generated from the first step were used for setting the high (that is, 'core') and low (that is, 'final') thresholds, respectively. Before thresholding, an isotropic Gaussian blur was applied (sigma of 1) and thresholds of 0.85 and 0.4 were chosen. Finally, objects smaller than 10% of the median cell volume (2,000 voxels) were discarded (same threshold we used for Cellpose).</p><p>Benchmarking. We compared the segmentations performed by Cellpose to segmentations performed by Stardist <ref type="bibr" target="#b18">18</ref> , Mask R-CNN <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17</ref> and U-Nets <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5</ref> .</p><p>Training two-and three-class U-Nets. To train U-Nets 3 , we followed the strategies from ref. <ref type="bibr" target="#b4">5</ref> . The two-class U-Net simply predicted pixels as inside/outside of cells, while the three-class U-Net predicted a third category of boundary pixels. Boundary pixels were defined as being less than N pixels away from border pixels and inside the cell, where N was adjusted on a per-image basis as 10% of the average ROI diameter. We used the standard U-Net architecture in all cases <ref type="bibr" target="#b2">3</ref> . At test time, we made ROI predictions by thresholding the 'inside' probability map and extracting all connected regions. For the three-class U-Net, we then added the boundary pixels to the nearest connected region 5 . To extend U-Nets to 3D, we applied them on 2D slices and then ran the connected region step on the entire concatenated 3D volume.</p><p>Training Stardist and Mask R-CNN. Stardist and Mask R-CNN were trained on the same training sets as Cellpose for the 'specialized' , 'generalized' and 'nuclei' datasets. 12% of the training set was used for validation for each algorithm. Stardist and Mask R-CNN were trained for 1,000 epochs. The learning rates were optimized to reduce the training error-this resulted in a learning rate of 0.0007 for Stardist and a learning rate of 0.001 for Mask R-CNN. For Mask R-CNN, TRAIN_ROIS_PER_IMAGE was increased to 300, MAX_GT_INSTANCES to 200 and DETECTION_MAX_INSTANCES to 400. Mask R-CNN was initialized with the pretrained 'imagenet' weights. All other parameters and learning schedules for both algorithms were kept to their default values.</p><p>Quantification of segmentation quality. We quantified the predictions of the algorithms by matching each predicted mask to the ground-truth mask that is most similar, as defined by the IoU. Then we evaluated the predictions at various levels of IoU; at a lower IoU, fewer pixels in a predicted mask have to match a corresponding ground-truth mask for a match to be considered valid. The valid matches define the true positives, TP, the masks with no valid matches are false positives, FP, and the ground-truth masks that have no valid match are false negatives, FN. Using these values, we computed the standard average precision metric (AP) for each image:</p><formula xml:id="formula_6">AP ¼ TP TP þ FP þ FN :</formula><p>The average precision reported is averaged over the average precision metric for each image in the test set. This is equivalent to using the 'matching_dataset' function from the Stardist package with the 'by_image' option set to True <ref type="bibr" target="#b60">60</ref> . We also quantified the segmentation quality using boundary prediction metrics <ref type="bibr" target="#b42">42</ref> and the Aggregated Jaccard Index <ref type="bibr" target="#b43">43</ref> . See the respective papers for the exact definitions.</p><p>Quantification of ROI properties. As a measure of convexity of each ROI we used the solidity index <ref type="bibr" target="#b61">61</ref> , which is equal to the ratio between the area of the ROI and the area of its convex hull. Each ground-truth ROI was matched at an IoU of 0.5 and the fraction of missed ROI for each convexity percentile was reported in Fig. <ref type="figure" target="#fig_4">5d</ref>, together with the average IoU of the matched ROI. Note that this analysis ignores false positives.</p><p>Within-image size variability. For each image, we computed the 25th and 75th percentiles of the distribution of pixel counts for all ROI in that image. The ratio of these numbers is between 0 and 1, and can be used as a measure of homogeneity, with a high number indicating a narrow distribution of sizes. We obtained a single homogeneity index per image for every image in our dataset and compared it with the segmentation performance of that image from the corresponding fold in the cross-validation where that image was a test image.</p><p>Statistical analyses. We performed Wilcoxon two-sided signed-rank tests in Extended Data Fig. <ref type="figure">6</ref> (n = 11 and 57 test images).</p><p>Extended Data Fig. <ref type="figure" target="#fig_0">1</ref> | Graphical user Interface (GuI). Shown in the GUI is an image from the test set, zoomed in on an area of interest, and segmented using Cellpose. The GUI serves two main purposes: 1) easily run Cellpose 'out-of-the-box' on new images and visualize the results in an interactive mode; 2) manually segment new images, to provide training data for Cellpose. The image view can be changed between image channels, cellpose vector flows and cellpose predicted pixel probabilities. Similarly, the mask overlays can be changed between outlines, transparent masks or both. The size calibration procedure can be run to estimate cell size, or the user can directly input the cell diameter, with an image overlay of a red disk shown as an aid for visual calibration. Dense, complete manual segmentations can be uploaded to our server with one button press, and the latest trained model can be downloaded from the drop-down menu.</p><p>Extended Data Fig. <ref type="figure" target="#fig_2">3</ref> | Prediction of median object diameter. a, The style vectors were used as linear regressors to predict the diameter of the objects in each image. Shown are the predictions for 68 test images (specialized and generalized data together), which were not used either for training cellpose or for training the size model. b, The refined size predictions are obtained after running cellpose on images resized according to the sizes computed in a. The median diameter of resulting objects is used as the refined size prediction for the final cellpose run. cd, Same as ab for the nucleus dataset.</p><p>Extended Data Fig. <ref type="figure">6</ref> | other performance metrics. ab, Boundary prediction metrics (precision, recall, F-score <ref type="bibr" target="#b42">42</ref> ) for: a, specialized and b, generalized data. All images of masks were first resized to an average ROI diameter of 30 pixels so that a common boundary width can be used across images. cd, Aggregated Jaccard Index </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc>Fig.1| Model architecture. a, Procedure for transforming manually annotated masks into a vector flow representation that can be predicted by a neural network. A simulated diffusion process starting from the center of the mask is used to derive spatial gradients that point toward the center of the cell, potentially indirectly around corners. The x and y gradients are combined in a single direction from 0° to 360°. b, Example spatial gradients for cells from the training dataset. c, A neural network is trained to predict the horizontal and vertical gradients, as well as whether a pixel belongs to any cell. The three predicted maps are combined into a gradient vector field. d, The details of the neural network that contains a standard backbone U-net<ref type="bibr" target="#b2">3</ref> to downsample and then upsample the feature maps, with skip connections between layers of the same size and global skip connections from the image styles, computed at the lowest resolution, to all successive computations. e, At test time, the predicted gradient vector fields are used to construct a dynamical system with fixed points whose basins of attraction represent the predicted masks. Informally, every pixel 'tracks the gradients' toward their eventual fixed point. f, All the pixels that converge to the same fixed point are assigned to the same mask.</figDesc><graphic coords="2,350.12,327.77,71.69,121.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 |</head><label>2</label><figDesc>Fig. 2 | Visualization of the diverse training dataset. The styles from the network for all the images in the cytoplasm dataset were embedded using t-SnE. Each point represents a different image. Legend is as follows: dark blue, Cell Image Library; blue, cytoplasm; cyan, membrane; green, nonfluorescent cells; orange, microscopy other and red, nonmicroscopy. Randomly chosen example images are shown around the t-SnE plot. Images with a second channel marking the nucleus are displayed in green/blue.</figDesc><graphic coords="3,43.04,55.67,248.04,248.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 |</head><label>3</label><figDesc>Fig. 3 | example Cellpose segmentations for 36 test images. The masks predicted by Cellpose are shown with dotted yellow lines. Compare these to the Stardist and Mask-R-Cnn in Extended Data Figs. 4 and 5.</figDesc><graphic coords="4,89.05,63.22,425.04,446.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Segmentation performance of specialist and generalist algorithms. a, Illustration of the training and testing data for specialist and generalist algorithms. We refer to the Cell Image Library dataset as a 'specialist dataset' due to the uniformity of its 100 images. b, Example test image segmentation for Cellpose, Mask R-Cnn and Stardist when trained as specialist models. The ground truth is shown as a blue line, while the model predictions are shown with a dotted yellow line. c, Example test image segmentation for the same models trained as generalist models. d, Quantified performance of Cellpose,Mask R-Cnn, Stardist, U-net3 and U-net2 models, trained as specialists and tested on specialized data (n = 11 test images). The IoU threshold quantifies the match between a predicted mask and a ground-truth mask, with 1 indicating a pixel-perfect match and 0.5 indicating as many correctly matched pixels as there were missed and false positive pixels. The average precision score is computed from the proportion of matched and missed masks. To gain an intuition for the range of these scores, refer to the reported values in b,c as well as Fig.3. e, Performance of the specialist models on generalized data (n = 57 test images). f,g, Same as d,e for generalist models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 |</head><label>5</label><figDesc>Fig. 5 | Model performance across image types and RoI statistics. a, Average precision for generalist models on subcategories of images from the test set (n = 17, 5, 9, 13, 13 test images). b, Left panels, example images with low and high homogeneity of ROI sizes, with ground truth masks in random colors. Right panel, Cellpose per-image performance as a function of size homogeneity. Red crosses indicate the example images from the left panels. c, Random samples of ROI masks divided into three subsets by convexity percentile. d, Left panel, fraction of missed ROI at an IoU threshold of 0.5 for generalized data (n = 57 test images). Right panel, average IoU of true positive ROI at IoU ≥ 0.5 for generalized data (n = 57 test images).</figDesc><graphic coords="6,56.85,275.40,64.35,64.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>43 for: c, specialized (n=11 test images) and d, generalized data (n=57 test images). **, p &lt; 0.01; ***, p &lt; 0.001; ****, p &lt; 0.0001; Wilcoxon two-sided signed-rank test. Extended Data Fig. 7 | Benchmarks for dataset of nuclei. a, Embedding of image styles for the nuclear dataset of 1139 images, with a new Cellpose model trained on this dataset. Legend: dark blue = Data Science Bowl dataset, blue = extra kaggle, cyan = ISBI 2009, green = MonuSeg. b, Segmentations on one example test image. c, Accuracy precision scores on test data for Cellpose, Mask R-Cnn, Stardist, unet3, and unet2 on n=111 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="12,43.24,55.56,508.80,344.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="18,51.26,55.56,260.52,260.64" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>NaTuRe MeThodS | VOL 18 | JAnUARy 2021 | 100-106 | www.nature.com/naturemethods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>NaTuRe MeThodS | www.nature.com/naturemethods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Extended Data Fig. 4 | example Stardist segmentations. Compare to Fig. 4. NaTuRe MeThodS | www.nature.com/naturemethods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Extended Data Fig. 5 | example Mask R-CNN segmentations. Compare to Fig. 4. NaTuRe MeThodS | www.nature.com/naturemethods</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Extended Data Fig. 8 | cellpose segmentations for nuclei. The predicted masks are shown in dotted yellow line. NaTuRe MeThodS | www.nature.com/naturemethods</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>acknowledgements</head><p>This research was funded by the <rs type="funder">Howard Hughes Medical Institute at the Janelia Research Campus</rs>. We thank <rs type="person">P. Tillberg</rs> and members of the <rs type="institution">Tillberg laboratory</rs> for advice related to expansion microscopy, and <rs type="person">W. Sun</rs> for sharing calcium imaging data from mouse hippocampus. We also thank <rs type="person">S. Saalfeld</rs> and <rs type="person">J. Funke</rs> for useful discussions.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>data availability</head><p>The manually segmented cytoplasmic dataset is available at <ref type="url" target="http://www.cellpose.org/dataset">www.cellpose.org/  dataset</ref> and <ref type="url" target="https://doi.org/10.25378/janelia.13270466">https://doi.org/10.25378/janelia.13270466</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>The reviewed version of Cellpose is available as Supplementary Software. The code, graphical user interface and updated versions are available at <ref type="url" target="http://www.github.com/mouseland/cellpose">www.github.com/  mouseland/cellpose</ref>. To test out the model directly on the web, visit <ref type="url" target="http://www.cellpose.org">www.cellpose.  org</ref>. Note that the test time augmentations and tiling, which improve segmentation, are not performed on the website to save computation time.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p><p>© The Author(s), under exclusive licence to Springer Nature America, Inc. 2020  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>additional information</head><p>Extended data is available for this paper at <ref type="url" target="https://doi.org/10.1038/s41592-020-01018-x">https://doi.org/10.1038/s41592-020-01018-x</ref>.</p><p>Supplementary information is available for this paper at <ref type="url" target="https://doi.org/10.1038/s41592-020-01018-x">https://doi.org/10.1038/  s41592-020-01018-x</ref>.</p><p>Correspondence and requests for materials should be addressed to M.P.</p><p>Reprints and permissions information is available at <ref type="url" target="http://www.nature.com/reprints">www.nature.com/reprints</ref>.</p><p>Peer review information Rita Strack was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Articles</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Microscopy-based high-content screening</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heigwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laufer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="1314" to="1325" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ilastik: interactive learning and segmentation toolkit</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>arXiv 1505.04597</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic neuron detection in calcium imaging data using convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Apthorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3270" to="3278" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiclass weighted loss for instance segmentation of cluttered cells</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Guerrero-Pena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 25th IEEE International Conference on Image Processing</title>
		<meeting>2018 25th IEEE International Conference on Image essing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2451" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Biomech. Biomed. Eng. Imaging Vis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="283" to="292" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A deep learning-based algorithm for 2-D cell segmentation in microscopy images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaltsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ilastik: interactive machine learning for (bio)image analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1226" to="1232" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fiji: an open-source platform for biological-image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schindelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="676" to="682" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cellprofiler 3.0: next-generation image processing for biology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">2005970</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Allen cell structure segmenter: a new open source toolkit for segmenting 3D intracellular structures in fluorescence microscopy images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1101/491035v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/491035v1" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A benchmark for epithelial cell tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kainmueller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11024-6_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11024-6_33" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Object-guided instance segmentation for biological images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.09199" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Mask</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.06870" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName><forename type="first">W</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/Mask_RCNN" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>GitHub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cells</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>fluorescent</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cell detection with star-convex polygons</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Broaddus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page">273</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A deep learning framework for nucleus segmentation using image style transfer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hollandi</surname></persName>
		</author>
		<idno type="DOI">10.1101/580605v1</idno>
		<ptr target="https://www.biorxiv.org/content/10.1101/580605v1" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Drake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CreateSpace</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Reference Manual</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Array programming with numpy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">SciPy: open source scientific tools for Python</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="http://www.scipy.org/" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>SciPy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Numba: a llvm-based python jit compiler</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seibert</surname></persName>
		</author>
		<idno type="DOI">10.1145/2833157.2833162</idno>
		<ptr target="https://doi.org/10.1145/2833157.2833162" />
	</analytic>
	<monogr>
		<title level="m">Proc. Second Workshop on the LLVM Compiler Infrastructure in HPC</title>
		<meeting>Second Workshop on the LLVM Compiler Infrastructure in HPC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The OpenCV Library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobb&apos;s J. Softw. Tools</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="125" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.01274" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rapid GUI Programming with Python and Qt: The Definitive Guide to PyQt Programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Summerfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Pearson Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scientific graphics and GUI library for Python</title>
		<author>
			<persName><forename type="first">L</forename><surname>Campagnola</surname></persName>
		</author>
		<ptr target="http://pyqtgraph.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
		<title level="m">Mathematical Morphology in Image Processing</title>
		<meeting><address><addrLine>Marcel Dekker</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="433" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmentation of touching cell nuclei using gradient flow tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Microsc</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">4401</biblScope>
			<biblScope unit="page">4410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Image data resource (IDR</title>
		<author>
			<persName><surname>Omero</surname></persName>
		</author>
		<ptr target="https://idr.openmicroscopy.org/cell/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image data resource: a bioimage data integration and publication platform</title>
		<author>
			<persName><forename type="first">E</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="775" to="781" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Lvd</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ccdb:6843, Mus musculus, neuroblastoma</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="DOI">10.7295/W9CCDB6843</idno>
		<ptr target="https://doi.org/10.7295/W9CCDB6843" />
	</analytic>
	<monogr>
		<title level="j">Cell Image Library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How cryo-em is revolutionizing structural biology</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>-C, Mcmullan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Scheres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Biochem. Sci</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Star-convex polyhedra for 3D object detection and segmentation in microscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Myers</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.03636" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An objective comparison of cell-tracking algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1141" to="1152" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualization and analysis of gene expression in tissue sections by spatial transcriptomics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ståhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="78" to="82" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matplotlib: a 2D graphics environment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Jupyter notebooks-a publishing format for reproducible computational workflows</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kluyver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELPUB</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Annotated high-throughput microscopy image sets for validation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ljosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Sokolnicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="637" to="637" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Voronoi-based segmentation of cells on image manifolds</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Computer Vision for Biomedical Image Applications</title>
		<imprint>
			<biblScope unit="page" from="535" to="543" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A novel validation algorithm allows for automated cell tracking and the extraction of biologically meaningful parameters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mamlouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schicktanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27315</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Micro-Net: a unified model for segmentation of various objects in microscopy images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="160" to="173" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Lopuhin</surname></persName>
		</author>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://github.com/lopuhin/kaggle-dsbowl-2018-dataset-fixes" />
		<imprint>
			<date type="published" when="2018">-dsbowl-2018-dataset-fixes (GitHub, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nuclear segmentation in microscope cell images: a hand-segmented dataset and comparison of algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="518" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Expansion microscopy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Tillberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Boyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">347</biblScope>
			<biblScope unit="page" from="543" to="548" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nanoscale imaging of RNA with expansion microscopy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="679" to="684" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>arXiv 1812.08008</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8837</biblScope>
			<biblScope unit="page">8845</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shape representation and classification using the poisson equation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1991" to="2005" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv 1603.05027</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">LinkNet: exploiting encoder representations for efficient semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 IEEE Visual Communications and Image Processing</title>
		<meeting>2017 IEEE Visual Communications and Image essing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">StarDist-object detection with star-convex shapes</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigert</surname></persName>
		</author>
		<ptr target="http://github.com/mpicbg-csbd/stardist" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>GitHub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A survey of shape feature extraction techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mingqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kidiyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="43" to="90" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
